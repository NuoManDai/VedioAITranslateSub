# * Settings marked with * are advanced settings that won't appear in the Streamlit page and can only be modified manually in config.py
# recommend to set in streamlit page
# -------------------
# version: "3.0.0"
# author: "Huanshere"
# -------------------

## ======================== Basic Settings ======================== ##

display_language: "zh-CN"

# API settings
api:
  key: ''
  base_url: 'https://api.siliconflow.cn'
  model: 'deepseek-ai/DeepSeek-V3.2'
  llm_support_json: true
# *Number of LLM multi-threaded accesses, set to 1 if using local LLM
max_workers: 4

# Language settings, written into the prompt, can be described in natural language
target_language: 'ÁÆÄ‰Ωì‰∏≠Êñá'

# Whether to use Demucs for vocal separation before transcription
demucs: true

whisper:
  # ["large-v3", "large-v3-turbo", "medium"]. Note: for zh model will force to use Belle/large-v3
  # large-v3 has highest accuracy, medium outputs more punctuation
  model: 'large-v3'
  # Whisper specified recognition language ISO 639-1
  language: 'ja'
  detected_language: 'ja'
  # Whisper running mode:
  # - "whisper": Native Faster-Whisper (recommended, like PotPlayer, best accuracy)
  # - "whisperx_local" / "local": WhisperX with pyannote VAD (legacy, may miss content)
  # - "cloud": WhisperX Cloud via 302.ai API
  # - "elevenlabs": ElevenLabs ASR API
  runtime: 'whisperx_local'
  # 302.ai API key
  whisperX_302_api_key: 'your_302_api_key'
  # ElevenLabs API key (experimental)
  elevenlabs_api_key: ''

  # ==================== High Accuracy Settings ====================
  # Beam search size: larger = more accurate but slower (default: 5, max: 15)
  beam_size: 15
  # Number of candidates for sampling (default: 5, max: 15)
  best_of: 15
  # Patience factor for beam search (default: 1.0, max: 3.0)
  patience: 3.0
  # Temperature for sampling, use multiple values for retry on failure
  temperatures: [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]
  # Initial prompt to guide transcription style and vocabulary
  # For Japanese anime: guides model to transcribe anime dialogue accurately
  # Leave empty for auto-generation (will auto-generate for Japanese)
  initial_prompt: ''

  # Log removed prompt-leak segments to CSV for debugging
  prompt_leak_log: true
  prompt_leak_log_path: 'output/log/prompt_leak_segments.txt'

  # ==================== Filter Thresholds (relax for fast speech) ====================
  # Compression ratio threshold: higher = allow more repetitive content (default: 2.4, relaxed: 2.8)
  compression_ratio_threshold: 2.8
  # Log probability threshold: lower = keep lower confidence transcriptions (default: -1.0, relaxed: -1.5)
  log_prob_threshold: -1.5
  # No speech threshold: lower = less likely to filter as silence (default: 0.6, relaxed: 0.4)
  no_speech_threshold: 0.4

  # ==================== VAD Settings (for WhisperX mode only) ====================
  # VAD onset threshold: lower = more sensitive to speech start (default: 0.5, short segments: 0.2)
  vad_onset: 0.12
  # VAD offset threshold: lower = more sensitive to speech end (default: 0.363, short segments: 0.1)
  vad_offset: 0.12
  method: whisperx_local
  whisper_x_302_api_key: ''

# Whether to burn subtitles into the video
burn_subtitles: true

## ======================== Advanced Settings ======================== ##
# *HuggingFace mirror endpoint for China users (leave empty to auto-detect)
hf_mirror: ''

# *HuggingFace token for speaker diarization (pyannote models)
# Get token from: https://huggingface.co/settings/tokens
# Also need to accept terms at: https://huggingface.co/pyannote/speaker-diarization-3.1
hf_token: ''

# *Enable speaker diarization (requires hf_token)
speaker_diarization: true

# Speaker diarization settings
diarization:
  # Minimum number of speakers (leave empty for auto-detection)
  # Set this if you know there are at least N speakers in the audio
  # Note: pyannote uses acoustic clustering, so it may not detect more speakers
  # than acoustically distinguishable voices (e.g., same voice actor = same speaker)
  min_speakers:
  # Maximum number of speakers (leave empty for auto-detection)
  max_speakers:
  # Enable speaker identification using voice fingerprinting
  # Requires reference audio samples in speaker_samples/ directory
  speaker_identification: true
  # Similarity threshold for speaker matching (0.0-1.0, higher = stricter)
  identification_threshold: 0.75
  # Use top-k nearest neighbors from vector DB (if enabled) for more stable matching
  identification_top_k: 5
  # Use multiple longest segments per speaker for embedding averaging
  identification_segment_top_n: 3
  # Minimum segment duration (seconds) used for embedding
  identification_segment_min_duration: 1.0
  # Auto-generate speaker samples from diarization when none exist
  auto_generate_samples: true
  samples_dir: 'speaker_samples'
  samples_per_speaker: 2
  sample_min_duration: 1.5
  sample_max_duration: 8.0

# *Speaker vector database for voiceprints
speaker_vector_db:
  enabled: true
  url: 'http://192.168.31.178:6333'
  collection: 'speaker_embeddings'

# *HTTP proxy for HuggingFace model downloads (e.g. http://127.0.0.1:10809)
# This proxy will be used to download models from HuggingFace when hf_mirror cannot access large files
http_proxy: 'http://127.0.0.1:7890'

# *üî¨ h264_nvenc GPU acceleration for ffmpeg, make sure your GPU supports it
ffmpeg_gpu: false

# *Youtube settings
youtube:
  cookies_path: ''

# *Default resolution for downloading YouTube videos [360, 1080, best]
ytb_resolution: '1080'

subtitle:
  # *Maximum length of each subtitle line in characters
  max_length: 75
  # *Translated subtitles are slightly larger than source subtitles, affecting the reference length for subtitle splitting
  target_multiplier: 1.2
  # *CJK mode: Use LLM sentence breaks for subtitle segmentation (recommended for Japanese/Chinese/Korean)
  # When enabled, subtitles will be split according to LLM's natural sentence breaks instead of WhisperX timestamps
  cjk_split: true

# *Summary length, set low to 2k if using local LLM
summary_length: 8000

# *Maximum number of words for the first rough cut, below 18 will cut too finely affecting translation, above 22 is too long and will make subsequent subtitle splitting difficult to align
max_split_length: 20

# *Whether to reflect the translation result in the original text
reflect_translate: true

# *Whether to pause after extracting professional terms and before translation, allowing users to manually adjust the terminology table output\log\terminology.json
pause_before_translate: false

## ======================== Dubbing Settings ======================== ##
# TTS selection [sf_fish_tts, openai_tts, gpt_sovits, azure_tts, fish_tts, edge_tts, custom_tts]
tts_method: 'azure_tts'

# SiliconFlow FishTTS
sf_fish_tts:
  # SiliconFlow API key
  api_key: 'YOUR_API_KEY'
  # only for mode "preset"
  voice: 'anna'
  # *only for mode "custom", dont set manually
  custom_name: ''
  voice_id: ''
  # preset, custom, dynamic
  mode: "preset"

# OpenAI TTS-1 API configuration, 302.ai API only
openai_tts:
  api_key: 'YOUR_302_API_KEY'
  voice: 'alloy'

# Azure configuration, 302.ai API only
azure_tts:
  api_key: 'YOUR_302_API_KEY'
  voice: 'zh-CN-YunfengNeural'

# FishTTS configuration, 302.ai API only
# ËßíËâ≤IDÂèØÂú® https://fish.audio/zh-CN/discovery/ Êü•ÁúãÂíåËé∑Âèñ
fish_tts:
  api_key: 'YOUR_302_API_KEY'
  character: 'Ê¥æËíô'
  character_id_dict:
    # Âä®Êº´ËßíËâ≤
    'Ê¥æËíô': 'd7de7164476248a38271f840f4426b0a'
    'ÈíüÁ¶ª': '2bc431816eba4b5c95d8e5e60b74add0'
    'ËÉ°Ê°É': 'ffecd524701a4dbe89adc5813ff9fbf7'
    'ÂàªÊô¥': 'cc6e9f3a12504f0aa7cc5d2073f03611'
    'ÁîòÈõ®': '66d54e9e0d9040b09f4c7ef2b9cb8cbc'
    'Èõ∑ÁîµÂ∞ÜÂÜõ': '574f86d5cf3c4f3b9b2de399af0d0ec8'
    'Á∫≥Ë•øÂ¶≤': 'b4b4b6d28fb743febd7c6a7f5c6e9f8e'
    'ËäôÂÆÅÂ®ú': '6e2f2c6d15d248f58e1a0a5ad79c0c5d'
    'ÊµÅËê§': 'd4d8c0b37f0e4e3ea5c1c2e1b5d8a9f7'
    'Ëä±ÁÅ´': 'a9e5c6f4b3d2419ea7c8d5b4e3f2a1b6'
    # ËµõÈ©¨Â®ò
    'ËµõÈ©¨Â®ò': '561fcedfdf0e4e1399d1bc4930d50c0e'
    # Áü•ÂêçÂ£∞‰ºò
    'Èõ∑ÂÜõ': 'aebaa2305aa2452fbdc8f41eec852a79'
    'Â§ÆËßÜÈÖçÈü≥': '59cb5986671546eaa6ca8ae6f29f6d22'

# SiliconFlow CosyVoice2 Clone
sf_cosyvoice2:
  api_key: 'YOUR_SF_KEY'

# Edge TTS configuration
edge_tts:
  voice: 'zh-CN-XiaoxiaoNeural'

# SoVITS configuration
gpt_sovits:
  character: 'Huanyuv2'
  refer_mode: 3

f5tts:
  302_api: 'YOUR_302_API_KEY'

# *Audio speed range
speed_factor:
  min: 1
  accept: 1.2 # Maximum acceptable speed
  max: 1.4

# *Merge audio configuration
min_subtitle_duration: 2.5 # Minimum subtitle duration, will be forcibly extended
min_trim_duration: 3.5 # Subtitles shorter than this value won't be split
tolerance: 1.5 # Allowed extension time to the next subtitle





## ======================== Additional settings ======================== ##

# Whisper model directory
model_dir: './_model_cache'

# Supported upload video formats
allowed_video_formats:
- 'mp4'
- 'mov'
- 'avi'
- 'mkv'
- 'flv'
- 'wmv'
- 'webm'

allowed_audio_formats:
- 'wav'
- 'mp3'
- 'flac'
- 'm4a'

# Spacy models
spacy_model_map:
  en: 'en_core_web_md'
  ru: 'ru_core_news_md'
  fr: 'fr_core_news_md'
  ja: 'ja_core_news_md'
  es: 'es_core_news_md'
  de: 'de_core_news_md'
  it: 'it_core_news_md'
  zh: 'zh_core_web_md'

# Languages that use space as separator
language_split_with_space:
- 'en'
- 'es'
- 'fr'
- 'de'
- 'it'
- 'ru'

# Languages that do not use space as separator
language_split_without_space:
- 'zh'
- 'ja'
source_language: ja
openai_tts_api_key: ''
openai_voice: alloy
azure_key: 'YOUR_302_API_KEY'
azure_voice: 'zh-CN-YunfengNeural'
fish_tts_api_key: ''
fish_tts_character: ''
sf_fish_tts_api_key: ''
sf_fish_tts_mode: preset
sf_fish_tts_voice: ''
sovits_character: ''
gpt_sovits_refer_mode: 3
edge_tts_voice: ''
sf_cosyvoice2_api_key: ''
f5tts_api_key: ''
