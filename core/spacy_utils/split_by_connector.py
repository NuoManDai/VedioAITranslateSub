import os
import warnings
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_COMMA_FILE, SPLIT_BY_CONNECTOR_FILE
from core.utils import rprint

warnings.filterwarnings("ignore", category=FutureWarning)

def analyze_connectors(doc, token):
    """
    Analyze whether a token is a connector that should trigger a sentence split.
    
    Processing logic and order:
     1. Check if the token is one of the target connectors based on the language.
     2. For 'that' (English), check if it's part of a contraction (e.g., that's, that'll).
     3. For all connectors, check if they function as a specific dependency of a verb or noun.
     4. Default to splitting for certain connectors if no other conditions are met.
     5. For coordinating conjunctions, check if they connect two independent clauses.
    """
    lang = doc.lang_
    if lang == "en":
        connectors = [
            # å› æœ
            "because", "since", "therefore", "thus", "hence", "so",
            # è½¬æŠ˜
            "but", "however", "although", "though", "yet", "while", "whereas",
            # å¹¶åˆ—
            "and", "or", "also", "moreover", "furthermore", "besides",
            # å…³ç³»
            "that", "which", "where", "when", "who", "whom", "whose",
            # æ¡ä»¶
            "if", "unless", "provided",
            # è®©æ­¥
            "even", "despite",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "zh":
        connectors = [
            # å› æœ
            "å› ä¸º", "æ‰€ä»¥", "å› æ­¤", "æ•…è€Œ", "äºæ˜¯", "ç”±äº",
            # è½¬æŠ˜
            "ä½†æ˜¯", "ç„¶è€Œ", "ä¸è¿‡", "å¯æ˜¯", "å´", "ä½†",
            # å¹¶åˆ—
            "è€Œä¸”", "å¹¶ä¸”", "åŒæ—¶", "å¦å¤–", "æ­¤å¤–", "è¿˜æœ‰",
            # æ¡ä»¶
            "å¦‚æœ", "å‡å¦‚", "è¦æ˜¯", "å€˜è‹¥", "è‹¥æ˜¯", "ä¸‡ä¸€",
            # è®©æ­¥
            "è™½ç„¶", "å°½ç®¡", "å³ä½¿", "å“ªæ€•", "çºµç„¶", "å°±ç®—",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "ja":
        connectors = [
            # é€†æ¥ (è½¬æŠ˜)
            "ã‘ã‚Œã©ã‚‚", "ã‘ã‚Œã©", "ã‘ã©", "ã—ã‹ã—", "ã ãŒ", "ã§ã‚‚", "ãŒ", 
            "ã¨ã“ã‚ãŒ", "ã«ã‚‚ã‹ã‹ã‚ã‚‰ãš", "ãã‚Œã§ã‚‚", "ãŸã ã—", "ã‚‚ã£ã¨ã‚‚",
            # å› æœ
            "ã ã‹ã‚‰", "ãã‚Œã§", "ã®ã§", "ã‹ã‚‰", "ãŸã‚", "ã—ãŸãŒã£ã¦", 
            "ãã®ãŸã‚", "ã‚ˆã£ã¦", "ã‚†ãˆã«", "ãªãœãªã‚‰",
            # å¹¶åˆ—/æ·»åŠ 
            "ãã—ã¦", "ã¾ãŸ", "ã•ã‚‰ã«", "ãã‚Œã‹ã‚‰", "ãŠã‚ˆã³", "ã‹ã¤", 
            "ã—ã‹ã‚‚", "ãã®ä¸Š", "åŠ ãˆã¦",
            # æ¡ä»¶
            "ãªã‚‰", "ãªã‚‰ã°", "ãŸã‚‰", "ã‚Œã°", "ã¨", "ã‚‚ã—",
            # è®©æ­¥
            "ã®ã«", "ã¦ã‚‚", "ã¨ã„ã£ã¦ã‚‚", "ã«ã—ã¦ã‚‚",
            # æ—¶é—´
            "ã¨ã", "ã¨ãã«", "éš›ã«", "ã‚ã¨", "ã¾ãˆ",
        ]
        mark_dep = "mark"
        det_pron_deps = ["case"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "fr":
        connectors = [
            # å› æœ
            "parce que", "car", "donc", "ainsi", "puisque",
            # è½¬æŠ˜
            "mais", "cependant", "pourtant", "toutefois", "nÃ©anmoins",
            # å¹¶åˆ—
            "et", "ou", "aussi", "de plus", "en outre",
            # å…³ç³»
            "que", "qui", "oÃ¹", "quand", "dont", "lequel",
            # æ¡ä»¶
            "si", "pourvu que", "Ã  condition que",
            # è®©æ­¥
            "bien que", "quoique", "mÃªme si",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "ru":
        connectors = [
            # å› æœ
            "Ğ¿Ğ¾Ñ‚Ğ¾Ğ¼Ñƒ Ñ‡Ñ‚Ğ¾", "Ğ¿Ğ¾ÑÑ‚Ğ¾Ğ¼Ñƒ", "Ñ‚Ğ°Ğº ĞºĞ°Ğº", "Ğ²ĞµĞ´ÑŒ", "Ğ¸Ğ±Ğ¾",
            # è½¬æŠ˜
            "Ğ½Ğ¾", "Ğ¾Ğ´Ğ½Ğ°ĞºĞ¾", "Ñ…Ğ¾Ñ‚Ñ", "Ğ²Ğ¿Ñ€Ğ¾Ñ‡ĞµĞ¼", "Ğ·Ğ°Ñ‚Ğ¾",
            # å¹¶åˆ—
            "Ğ¸", "Ğ¸Ğ»Ğ¸", "Ñ‚Ğ°ĞºĞ¶Ğµ", "ĞºÑ€Ğ¾Ğ¼Ğµ Ñ‚Ğ¾Ğ³Ğ¾", "Ğ¿Ñ€Ğ¸Ñ‚Ğ¾Ğ¼",
            # å…³ç³»
            "Ñ‡Ñ‚Ğ¾", "ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹", "Ğ³Ğ´Ğµ", "ĞºĞ¾Ğ³Ğ´Ğ°", "Ñ‡ĞµĞ¹",
            # æ¡ä»¶
            "ĞµÑĞ»Ğ¸", "Ğ¿Ñ€Ğ¸ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ğ¸",
            # è®©æ­¥
            "Ğ½ĞµÑĞ¼Ğ¾Ñ‚Ñ€Ñ Ğ½Ğ°", "Ñ…Ğ¾Ñ‚Ñ", "Ğ´Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "es":
        connectors = [
            # å› æœ
            "porque", "por eso", "asÃ­ que", "ya que", "puesto que",
            # è½¬æŠ˜
            "pero", "sin embargo", "aunque", "no obstante",
            # å¹¶åˆ—
            "y", "o", "tambiÃ©n", "ademÃ¡s", "asimismo",
            # å…³ç³»
            "que", "cual", "donde", "cuando", "quien", "cuyo",
            # æ¡ä»¶
            "si", "a menos que", "con tal de que",
            # è®©æ­¥
            "aunque", "a pesar de que", "si bien",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "de":
        connectors = [
            # å› æœ
            "weil", "denn", "deshalb", "daher", "darum",
            # è½¬æŠ˜
            "aber", "jedoch", "obwohl", "trotzdem", "dennoch",
            # å¹¶åˆ—
            "und", "oder", "auch", "auÃŸerdem", "ferner",
            # å…³ç³»
            "dass", "welche", "wo", "wann", "wer", "dessen",
            # æ¡ä»¶
            "wenn", "falls", "sofern",
            # è®©æ­¥
            "obwohl", "obgleich", "selbst wenn",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    elif lang == "it":
        connectors = [
            # å› æœ
            "perchÃ©", "quindi", "perciÃ²", "poichÃ©", "siccome",
            # è½¬æŠ˜
            "ma", "perÃ²", "tuttavia", "sebbene", "benchÃ©",
            # å¹¶åˆ—
            "e", "o", "anche", "inoltre", "pure",
            # å…³ç³»
            "che", "quale", "dove", "quando", "chi", "cui",
            # æ¡ä»¶
            "se", "qualora", "purchÃ©",
            # è®©æ­¥
            "anche se", "nonostante", "malgrado",
        ]
        mark_dep = "mark"
        det_pron_deps = ["det", "pron"]
        verb_pos = "VERB"
        noun_pos = ["NOUN", "PROPN"]
    else:
        return False, False
    
    if token.text.lower() not in connectors:
        return False, False
    
    if lang == "en" and token.text.lower() == "that":
        if token.dep_ == mark_dep and token.head.pos_ == verb_pos:
            return True, False
        else:
            return False, False
    elif token.dep_ in det_pron_deps and token.head.pos_ in noun_pos:
        return False, False
    else:
        return True, False

def split_by_connectors(text, context_words=5, nlp=None):
    doc = nlp(text)
    sentences = [doc.text]  # init
    
    while True:
        # Handle each task with a single cut
        # avoiding the fragmentation of a sentence into multiple parts at the same time.
        split_occurred = False
        new_sentences = []
        
        for sent in sentences:
            doc = nlp(sent)
            start = 0
            
            for i, token in enumerate(doc):
                split_before, _ = analyze_connectors(doc, token)
                
                if i + 1 < len(doc) and doc[i + 1].text in ["'s", "'re", "'ve", "'ll", "'d"]:
                    continue
                
                left_words = doc[max(0, token.i - context_words):token.i]
                right_words = doc[token.i+1:min(len(doc), token.i + context_words + 1)]
                
                left_words = [word.text for word in left_words if not word.is_punct]
                right_words = [word.text for word in right_words if not word.is_punct]
                
                if len(left_words) >= context_words and len(right_words) >= context_words and split_before:
                    rprint(f"[yellow]âœ‚ï¸  Split before '{token.text}': {' '.join(left_words)}| {token.text} {' '.join(right_words)}[/yellow]")
                    new_sentences.append(doc[start:token.i].text.strip())
                    start = token.i
                    split_occurred = True
                    break
            
            if start < len(doc):
                new_sentences.append(doc[start:].text.strip())
        
        if not split_occurred:
            break
        
        sentences = new_sentences
    
    return sentences

def split_sentences_main(nlp):
    # Read input sentences
    with open(SPLIT_BY_COMMA_FILE, "r", encoding="utf-8") as input_file:
        sentences = input_file.readlines()
    
    all_split_sentences = []
    # Process each input sentence
    for sentence in sentences:
        split_sentences = split_by_connectors(sentence.strip(), nlp = nlp)
        all_split_sentences.extend(split_sentences)
    
    with open(SPLIT_BY_CONNECTOR_FILE, "w+", encoding="utf-8") as output_file:
        for sentence in all_split_sentences:
            output_file.write(sentence + "\n")
        # do not add a newline at the end of the file
        output_file.seek(output_file.tell() - 1, os.SEEK_SET)
        output_file.truncate()

    # ä¿ç•™ä¸­é—´æ–‡ä»¶ç”¨äºè°ƒè¯•
    # os.remove(SPLIT_BY_COMMA_FILE)
    
    rprint(f"[green]ğŸ’¾ Sentences split by connectors saved to â†’  `{SPLIT_BY_CONNECTOR_FILE}`[/green]")

if __name__ == "__main__":
    nlp = init_nlp()
    split_sentences_main(nlp)
    # nlp = init_nlp()
    # a = "and show the specific differences that make a difference between a breakaway that results in a goal in the NHL versus one that doesn't."
    # print(split_by_connectors(a, nlp))