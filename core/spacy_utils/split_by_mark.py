import os
import re
import pandas as pd
import warnings
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_MARK_FILE
from core.utils.config_utils import load_key, get_joiner
from rich import print as rprint

warnings.filterwarnings("ignore", category=FutureWarning)

# Japanese sentence-ending particles and expressions
JA_SENTENCE_ENDERS = [
    # Polite endings (most reliable)
    'ã¾ã™', 'ã§ã™', 'ã¾ã—ãŸ', 'ã§ã—ãŸ', 'ã¾ã›ã‚“', 'ãã ã•ã„', 'ã¾ã—ã‚‡ã†',
    # Plain verb endings
    'ã ', 'ãŸ', 'ã‚‹',
    # Sentence-final particles
    'ã­', 'ã‚ˆ', 'ã‚', 'ã', 'ãª', 'ã•', 'ã®', 'ã‹',
    # Combined particles
    'ã‹ãª', 'ã‹ã­', 'ã‹ã—ã‚‰', 'ã‚ˆã­', 'ã®ã‚ˆ', 'ã‚ã‚ˆ',
    # Explanatory forms
    'ã‚“ã ', 'ã®ã ', 'ã‚“ã§ã™', 'ã®ã§ã™',
    # Quotative
    'ã¨',
]

def split_japanese_by_time_and_grammar(chunks_df, nlp, gap_threshold=0.3):
    """
    Split Japanese text by combining time gaps and grammatical patterns.
    Since Japanese ASR often lacks punctuation, we use:
    1. Time gaps between chunks (indicating pauses)
    2. Sentence-ending particles and verb forms
    3. Punctuation marks (! ? etc.)
    """
    chunks = chunks_df.copy()
    chunks['text'] = chunks['text'].apply(lambda x: str(x).strip('"').strip('"').replace('"', '').strip())
    
    # Calculate time gaps
    chunks['gap'] = chunks['start'].shift(-1) - chunks['end']
    
    sentences = []
    current_sentence = []
    current_start = None
    
    # é«˜å¯é æ€§æ•¬è¯­å¥å°¾ - è¿™äº›å‡ ä¹æ€»æ˜¯è¡¨ç¤ºå¥å­ç»“æŸ
    POLITE_ENDERS = ['ã¾ã™', 'ã§ã™', 'ã¾ã—ãŸ', 'ã§ã—ãŸ', 'ã¾ã›ã‚“', 'ãã ã•ã„', 'ã¾ã—ã‚‡ã†']
    # æ™®é€šä½“å¥å°¾ - ä¹Ÿæ¯”è¾ƒå¯é 
    PLAIN_ENDERS = ['ã ', 'ãŸ', 'ã‚‹', 'ã„', 'ãªã„', 'ã‚ã‚‹', 'ã„ã‚‹', 'ãˆã‚‹', 'ã‚ã‚‹', 'ã®ã ', 'ã‚“ã ']
    # å¥å°¾åŠ©è¯ - éœ€è¦é…åˆæ—¶é—´é—´éš”ä½¿ç”¨
    PARTICLE_ENDERS = ['ã­', 'ã‚ˆ', 'ã‚', 'ã', 'ãª', 'ã•', 'ã®', 'ã‹', 'ã‚ˆã­', 'ã‹ãª', 'ã‹ã­']
    
    for i, row in chunks.iterrows():
        if current_start is None:
            current_start = row['start']
        
        current_sentence.append(row['text'])
        current_text = ''.join(current_sentence)
        
        should_split = False
        
        # Check for punctuation marks (highest priority)
        if row['text'] in ['!', '?', 'ï¼Ÿ', 'ï¼', 'ã€‚', 'ï¼']:
            should_split = True
        # Check for significant time gap (strong indicator of sentence boundary)
        elif pd.notna(row['gap']) and row['gap'] > gap_threshold:
            should_split = True
        # é«˜å¯é æ€§æ•¬è¯­å¥å°¾ - ä¸éœ€è¦ gap ä¹Ÿå¯ä»¥åˆ†å‰²ï¼ˆæœ€å°é•¿åº¦ 5 å­—ç¬¦é¿å…è¯¯åˆ¤ï¼‰
        elif len(current_text) >= 5:
            for ender in POLITE_ENDERS:
                if current_text.endswith(ender):
                    should_split = True
                    break
        # æ™®é€šä½“å¥å°¾é…åˆå°é—´éš”
        if not should_split and len(current_text) >= 8 and pd.notna(row['gap']) and row['gap'] > 0.05:
            for ender in PLAIN_ENDERS:
                if current_text.endswith(ender):
                    should_split = True
                    break
        # å¥å°¾åŠ©è¯éœ€è¦é…åˆæ—¶é—´é—´éš”ï¼ˆé¿å…è¯¯åˆ¤ï¼‰
        if not should_split and len(current_text) >= 3 and pd.notna(row['gap']) and row['gap'] > 0.08:
            for ender in PARTICLE_ENDERS:
                if current_text.endswith(ender):
                    should_split = True
                    break
        
        if should_split and current_sentence:
            sentences.append(''.join(current_sentence))
            current_sentence = []
            current_start = None
    
    # Add remaining text
    if current_sentence:
        sentences.append(''.join(current_sentence))
    
    # Post-process: merge short sentences with previous (not next)
    merged_sentences = []
    for sent in sentences:
        sent = sent.strip()
        # å¦‚æœå½“å‰å¥å­å¤ªçŸ­(<=4å­—ç¬¦)ä¸”æœ‰å‰ä¸€å¥ï¼Œåˆå¹¶åˆ°å‰ä¸€å¥
        if len(sent) <= 4 and merged_sentences:
            merged_sentences[-1] = merged_sentences[-1] + sent
        else:
            merged_sentences.append(sent)
    
    rprint(f"[blue]ğŸ“Š Japanese split: {len(merged_sentences)} sentences (gap threshold: {gap_threshold}s)[/blue]")
    return merged_sentences

def split_by_mark(nlp):
    whisper_language = load_key("whisper.language")
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language # consider force english case
    joiner = get_joiner(language)
    rprint(f"[blue]ğŸ” Using {language} language joiner: '{joiner}'[/blue]")
    chunks = pd.read_excel("output/log/cleaned_chunks.xlsx")
    
    # Special handling for Japanese - use time-based splitting due to lack of punctuation in ASR
    if language == 'ja':
        sentences_by_mark = split_japanese_by_time_and_grammar(chunks, nlp)
    else:
        # Clean text: remove surrounding quotes and any embedded quotes
        chunks.text = chunks.text.apply(lambda x: str(x).strip('"').strip('"').replace('"', '').strip())
        
        # join with joiner
        input_text = joiner.join(chunks.text.to_list())

        doc = nlp(input_text)
        assert doc.has_annotation("SENT_START")

        # skip - and ...
        sentences_by_mark = []
        current_sentence = []
        
        # iterate all sentences
        for sent in doc.sents:
            text = sent.text.strip()
            
            # check if the current sentence ends with - or ...
            if current_sentence and (
                text.startswith('-') or 
                text.startswith('...') or
                current_sentence[-1].endswith('-') or
                current_sentence[-1].endswith('...')
            ):
                current_sentence.append(text)
            else:
                if current_sentence:
                    sentences_by_mark.append(joiner.join(current_sentence))
                    current_sentence = []
                current_sentence.append(text)
        
        # add the last sentence
        if current_sentence:
            sentences_by_mark.append(joiner.join(current_sentence))

    rprint(f"[blue]ğŸ“Š Split into {len(sentences_by_mark)} sentences[/blue]")
    
    with open(SPLIT_BY_MARK_FILE, "w", encoding="utf-8") as output_file:
        for i, sentence in enumerate(sentences_by_mark):
            if i > 0 and sentence.strip() in [',', '.', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼']:
                # ! If the current line contains only punctuation, merge it with the previous line, this happens in Chinese, Japanese, etc.
                output_file.seek(output_file.tell() - 1, os.SEEK_SET)  # Move to the end of the previous line
                output_file.write(sentence)  # Add the punctuation
            else:
                output_file.write(sentence + "\n")
    
    rprint(f"[green]ğŸ’¾ Sentences split by punctuation marks saved to â†’  `{SPLIT_BY_MARK_FILE}`[/green]")

if __name__ == "__main__":
    nlp = init_nlp()
    split_by_mark(nlp)
