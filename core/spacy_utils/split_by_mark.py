import os
import re
import pandas as pd
import warnings
from core.spacy_utils.load_nlp_model import init_nlp, SPLIT_BY_MARK_FILE
from core.utils.config_utils import load_key, get_joiner
from rich import print as rprint

warnings.filterwarnings("ignore", category=FutureWarning)

def split_sentence_by_time_gap(sentence: str, chunks: pd.DataFrame, joiner: str, threshold: float) -> list[str]:
    """
    å¯¹å•ä¸ªå¥å­ï¼Œæ£€æŸ¥å…¶å†…éƒ¨æ˜¯å¦æœ‰è¶…è¿‡é˜ˆå€¼çš„æ—¶é—´é—´éš”ï¼Œå¦‚æœæœ‰åˆ™åˆ‡åˆ†ã€‚
    é€šè¿‡é€å­—ç¬¦åŒ¹é…æŠŠå¥å­æ˜ å°„å› chunksï¼Œç„¶åæ£€æŸ¥æ—¶é—´ä¿¡æ¯ã€‚
    """
    if not sentence.strip():
        return [sentence]
    
    # æ‰¾åˆ°è¿™ä¸ªå¥å­å¯¹åº”çš„ chunks èŒƒå›´
    # ç­–ç•¥ï¼šæŒ‰é¡ºåºåŒ¹é… chunks çš„ text
    result_segments = []
    current_segment = []
    
    # è®¡ç®—æ—¶é—´ä¿¡æ¯
    chunks_copy = chunks.copy()
    chunks_copy['duration'] = chunks_copy['end'] - chunks_copy['start']
    chunks_copy['gap_to_next'] = chunks_copy['start'].shift(-1) - chunks_copy['end']
    
    # æŠŠå¥å­æ‹†æˆå­—ç¬¦ï¼Œé€ä¸ªåŒ¹é… chunks
    remaining_sentence = sentence
    
    for idx, row in chunks_copy.iterrows():
        chunk_text = str(row['text']).strip('"').strip('"').replace('"', '').strip()
        duration = row.get('duration', 0)
        gap_to_next = row.get('gap_to_next', 0)
        
        # æ£€æŸ¥è¿™ä¸ª chunk æ˜¯å¦åœ¨å½“å‰å¥å­ä¸­
        if joiner:
            # æœ‰è¿æ¥ç¬¦çš„è¯­è¨€ï¼ˆå¦‚è‹±è¯­ï¼‰ï¼Œchunk å¯èƒ½å¸¦ç©ºæ ¼
            if remaining_sentence.startswith(chunk_text):
                remaining_sentence = remaining_sentence[len(chunk_text):].lstrip()
                current_segment.append(chunk_text)
            elif remaining_sentence.startswith(chunk_text + joiner):
                remaining_sentence = remaining_sentence[len(chunk_text + joiner):]
                current_segment.append(chunk_text)
            else:
                continue  # è¿™ä¸ª chunk ä¸åœ¨å½“å‰å¥å­ä¸­
        else:
            # æ— è¿æ¥ç¬¦çš„è¯­è¨€ï¼ˆå¦‚æ—¥è¯­ã€ä¸­æ–‡ï¼‰
            if chunk_text in remaining_sentence:
                pos = remaining_sentence.find(chunk_text)
                if pos == 0:
                    remaining_sentence = remaining_sentence[len(chunk_text):]
                    current_segment.append(chunk_text)
                else:
                    continue
            else:
                continue
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åœ¨æ­¤å¤„åˆ‡åˆ†
        should_split = False
        if duration > threshold:
            should_split = True
        elif pd.notna(gap_to_next) and gap_to_next > threshold:
            should_split = True
        
        if should_split and remaining_sentence:  # è¿˜æœ‰å‰©ä½™å†…å®¹æ‰åˆ‡åˆ†
            result_segments.append(joiner.join(current_segment))
            current_segment = []
    
    # æ·»åŠ æœ€åä¸€æ®µ
    if current_segment:
        result_segments.append(joiner.join(current_segment))
    
    # å¦‚æœæ²¡æœ‰åŒ¹é…æˆåŠŸï¼Œè¿”å›åŸå¥å­
    if not result_segments:
        return [sentence]
    
    return result_segments


def split_by_mark(nlp):
    whisper_language = load_key("whisper.language")
    language = load_key("whisper.detected_language") if whisper_language == 'auto' else whisper_language
    joiner = get_joiner(language)
    rprint(f"[blue]ğŸ” Using {language} language joiner: '{joiner}'[/blue]")
    segments_path = "output/log/segments.xlsx"
    chunks_path = "output/log/cleaned_chunks.xlsx"

    def has_punctuation(text_list: list) -> bool:
        punctuation_marks = {"ã€‚", "ã€", "ï¼", "ï¼Ÿ", ".", ",", "!", "?", "ï¼›", "ï¼š", ";", ":"}
        if not text_list:
            return False
        lines_with_punc = 0
        for item in text_list:
            if any(mark in item for mark in punctuation_marks):
                lines_with_punc += 1
        ratio = lines_with_punc / max(len(text_list), 1)
        return ratio >= 0.2

    if os.path.exists(segments_path):
        segments_df = pd.read_excel(segments_path)
        if "text" not in segments_df.columns:
            raise ValueError("segments.xlsx missing required column: text")
        base_sentences = segments_df["text"].astype(str).tolist()
        use_segments_base = not has_punctuation(base_sentences)
        if use_segments_base:
            rprint("[blue]ğŸ§© Using segments.xlsx as sentence base (no punctuation detected)[/blue]")
        else:
            rprint("[blue]ğŸ§© segments.xlsx has punctuation, using spaCy split[/blue]")
    else:
        chunks = pd.read_excel(chunks_path)
        base_sentences = chunks["text"].astype(str).tolist()
        rprint("[yellow]âš ï¸ segments.xlsx not found, fallback to cleaned_chunks.xlsx[/yellow]")
        use_segments_base = False
    
    # ä»é…ç½®è¯»å–æ—¶é—´é—´éš”é˜ˆå€¼
    try:
        time_gap_threshold = load_key("time_gap_threshold")
    except KeyError:
        time_gap_threshold = None
    
    if time_gap_threshold and time_gap_threshold > 0:
        rprint(f"[blue]â±ï¸ Time gap threshold enabled: {time_gap_threshold}s[/blue]")
    
    base_sentences = [
        str(text).strip('"').strip('"').replace('"', '').strip() for text in base_sentences
    ]

    if use_segments_base:
        sentences_by_mark = [sentence for sentence in base_sentences if sentence.strip()]
        rprint(f"[blue]ğŸ“Š Using {len(sentences_by_mark)} segment sentences directly[/blue]")
    else:
        # Step 1: å…ˆç”¨ spaCy æ ¹æ®æ ‡ç‚¹åˆ‡åˆ†
        full_text = joiner.join(base_sentences)
        doc = nlp(full_text)
        assert doc.has_annotation("SENT_START")

        # å¤„ç† - å’Œ ... çš„æƒ…å†µ
        spacy_sentences = []
        current_sentence = []
        
        for sent in doc.sents:
            text = sent.text.strip()
            
            if current_sentence and (
                text.startswith('-') or 
                text.startswith('...') or
                current_sentence[-1].endswith('-') or
                current_sentence[-1].endswith('...')
            ):
                current_sentence.append(text)
            else:
                if current_sentence:
                    spacy_sentences.append(joiner.join(current_sentence))
                    current_sentence = []
                current_sentence.append(text)
        
        if current_sentence:
            spacy_sentences.append(joiner.join(current_sentence))
        
        rprint(f"[blue]ğŸ“Š spaCy split into {len(spacy_sentences)} sentences[/blue]")
        
        # Step 2: å¦‚æœå¯ç”¨äº†æ—¶é—´åˆ‡åˆ†ï¼Œå¯¹æ¯ä¸ª spaCy å¥å­å†æ£€æŸ¥æ—¶é—´é—´éš”
        sentences_by_mark = []
        
        if time_gap_threshold and time_gap_threshold > 0:
            # éœ€è¦é‡æ–°åŠ è½½åŸå§‹ chunksï¼ˆå¸¦æ—¶é—´ä¿¡æ¯ï¼‰
            chunks_with_time = pd.read_excel(chunks_path)
            chunks_with_time['duration'] = chunks_with_time['end'] - chunks_with_time['start']
            chunks_with_time['gap_to_next'] = chunks_with_time['start'].shift(-1) - chunks_with_time['end']
            
            # ç”¨ä¸€ä¸ªå…¨å±€æŒ‡é’ˆè¿½è¸ªå½“å‰å¤„ç†åˆ°å“ªä¸ª chunk
            chunk_idx = 0
            total_chunks = len(chunks_with_time)
            time_split_count = 0
            
            for sentence in spacy_sentences:
                if not sentence.strip():
                    continue
                
                # æ”¶é›†å½“å‰å¥å­å¯¹åº”çš„ chunks åŠå…¶æ—¶é—´ä¿¡æ¯
                current_segment = []
                sentence_remaining = sentence
                
                while chunk_idx < total_chunks and sentence_remaining:
                    row = chunks_with_time.iloc[chunk_idx]
                    chunk_text = str(row['text']).strip('"').strip('"').replace('"', '').strip()
                    duration = row['duration']
                    gap_to_next = row['gap_to_next'] if pd.notna(row['gap_to_next']) else 0
                    
                    # æ£€æŸ¥è¿™ä¸ª chunk æ˜¯å¦åœ¨å‰©ä½™å¥å­ä¸­
                    if joiner:
                        check_text = chunk_text
                        if sentence_remaining.startswith(check_text):
                            sentence_remaining = sentence_remaining[len(check_text):].lstrip()
                            current_segment.append(chunk_text)
                            chunk_idx += 1
                        else:
                            break  # å½“å‰ chunk ä¸åŒ¹é…ï¼Œè¯´æ˜å¥å­å¤„ç†å®Œäº†
                    else:
                        # æ—¥è¯­/ä¸­æ–‡ç­‰æ— ç©ºæ ¼è¯­è¨€
                        if sentence_remaining.startswith(chunk_text):
                            sentence_remaining = sentence_remaining[len(chunk_text):]
                            current_segment.append(chunk_text)
                            chunk_idx += 1
                        else:
                            break
                    
                    # æ£€æŸ¥æ˜¯å¦éœ€è¦åœ¨æ­¤å¤„åˆ‡åˆ†ï¼ˆè¿˜æœ‰å‰©ä½™å†…å®¹æ‰åˆ‡ï¼‰
                    should_split = (duration > time_gap_threshold or gap_to_next > time_gap_threshold)
                    
                    if should_split and sentence_remaining:
                        sentences_by_mark.append(joiner.join(current_segment))
                        current_segment = []
                        time_split_count += 1
                
                # æ·»åŠ å½“å‰å¥å­çš„æœ€åä¸€æ®µ
                if current_segment:
                    sentences_by_mark.append(joiner.join(current_segment))
            
            if time_split_count > 0:
                rprint(f"[blue]â±ï¸ Time gap made {time_split_count} additional cuts[/blue]")
        else:
            sentences_by_mark = spacy_sentences

    rprint(f"[blue]ğŸ“Š Final: {len(sentences_by_mark)} sentences[/blue]")
    
    with open(SPLIT_BY_MARK_FILE, "w", encoding="utf-8") as output_file:
        for i, sentence in enumerate(sentences_by_mark):
            if i > 0 and sentence.strip() in [',', '.', 'ï¼Œ', 'ã€‚', 'ï¼Ÿ', 'ï¼']:
                # ! If the current line contains only punctuation, merge it with the previous line, this happens in Chinese, Japanese, etc.
                output_file.seek(output_file.tell() - 1, os.SEEK_SET)  # Move to the end of the previous line
                output_file.write(sentence)  # Add the punctuation
            else:
                output_file.write(sentence + "\n")
    
    rprint(f"[green]ğŸ’¾ Sentences split by punctuation marks saved to â†’  `{SPLIT_BY_MARK_FILE}`[/green]")

if __name__ == "__main__":
    nlp = init_nlp()
    split_by_mark(nlp)
