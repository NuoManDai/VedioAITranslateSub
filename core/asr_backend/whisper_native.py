"""
ä½¿ç”¨ Silero VAD çš„åŸç”Ÿ Faster-Whisper è½¬å†™ã€‚
æµç¨‹ç±»ä¼¼ PotPlayer/Faster-Whisper-XXLï¼š
- Silero VAD è¿›è¡Œè¯­éŸ³æ´»åŠ¨æ£€æµ‹
- å¥å­çº§åå¤„ç†
- ç›´æ¥è°ƒç”¨ faster-whisper è¾“å‡ºè¯çº§æ—¶é—´æˆ³

æ³¨æ„ï¼šä¸è¦ç”¨ librosa è¯»éŸ³é¢‘ï¼ä¼šå¯¼è‡´è¯†åˆ«é”™è¯¯ã€‚
faster-whisper å†…éƒ¨ç”¨ PyAVï¼Œèƒ½æ­£ç¡®å¤„ç†éŸ³é¢‘ã€‚
"""
import os
import re
import time
import torch
from faster_whisper import WhisperModel
from rich import print as rprint
from core.utils import *
from core.asr_backend._common import get_language_prompt, select_vad_parameters

MODEL_DIR = load_key("model_dir")

# -------------------------
# å¥å­è¾¹ç•Œè§„åˆ™ï¼ˆç±»ä¼¼ Faster-Whisper-XXL --sentenceï¼‰
# -------------------------
# å¥æœ«æ ‡ç‚¹
SENTENCE_ENDINGS = re.compile(r'[.!?ã€‚ï¼ï¼Ÿ]$')
# å¥æœ«çœç•¥å·ï¼ˆå…è®¸åˆ‡åˆ†ï¼‰
ELLIPSIS_END = re.compile(r'\.{3}$|â€¦$')
# åˆ†å¥æ—¶å¿½ç•¥è¿™äº›ç¼©å†™
ABBREVIATIONS = {'Mr.', 'Mrs.', 'Ms.', 'Dr.', 'Prof.', 'Sr.', 'Jr.', 'etc.', 'vs.'}


def split_into_sentences(segments, max_gap=0.5):
    """
    å°†åˆ†æ®µåå¤„ç†ä¸ºæ›´è‡ªç„¶çš„å¥å­ï¼Œç±»ä¼¼ Faster-Whisper-XXL çš„ --sentenceã€‚
    
    å‚æ•°ï¼š
        segments: å¸¦è¯çº§æ—¶é—´æˆ³çš„åˆ†æ®µåˆ—è¡¨
        max_gap: åŒä¸€å¥å†…å…è®¸çš„æœ€å¤§è¯é—´éš”ï¼ˆç§’ï¼‰
    
    è¿”å›ï¼š
        å¥å­çº§åˆ†æ®µåˆ—è¡¨
    """
    if not segments:
        return segments
    
    sentences = []
    current_sentence = None
    
    for seg in segments:
        if not seg.get('words'):
            # æ²¡æœ‰è¯çº§æ—¶é—´æˆ³ï¼Œç›´æ¥ä½¿ç”¨åˆ†æ®µ
            sentences.append(seg)
            continue
        
        for word_info in seg['words']:
            word = word_info['word'].strip()
            if not word:
                continue
            
            # éœ€è¦æ—¶å¼€å§‹æ–°å¥å­
            if current_sentence is None:
                current_sentence = {
                    'start': word_info['start'],
                    'end': word_info['end'],
                    'text': word,
                    'words': [word_info]
                }
            else:
                # åˆ¤æ–­æ˜¯å¦å¼€å§‹æ–°å¥
                gap = word_info['start'] - current_sentence['end']
                prev_text = current_sentence['text']
                
                # å¼€å¯æ–°å¥çš„æ¡ä»¶ï¼š
                # 1. è¯é—´éš”å¾ˆå¤§
                # 2. ä¸Šä¸€å¥ä»¥å¥æœ«æ ‡ç‚¹ç»“æŸï¼ˆä¸”ä¸æ˜¯ç¼©å†™ï¼‰
                should_split = False
                
                if gap > max_gap:
                    should_split = True
                elif SENTENCE_ENDINGS.search(prev_text):
                    # ç¡®ä¿ä¸æ˜¯ç¼©å†™
                    last_word = prev_text.split()[-1] if prev_text.split() else ''
                    if last_word not in ABBREVIATIONS:
                        # çœç•¥å·ä»…å…è®¸åœ¨åˆ†æ®µè¾¹ç•Œåˆ‡åˆ†
                        if not ELLIPSIS_END.search(prev_text):
                            should_split = True
                
                if should_split:
                    # ä¿å­˜å½“å‰å¥
                    sentences.append(current_sentence)
                    # å¼€å§‹æ–°å¥
                    current_sentence = {
                        'start': word_info['start'],
                        'end': word_info['end'],
                        'text': word,
                        'words': [word_info]
                    }
                else:
                    # ç»§ç»­å½“å‰å¥
                    current_sentence['end'] = word_info['end']
                    current_sentence['text'] += word
                    current_sentence['words'].append(word_info)
    
    # è¿½åŠ æœ€åä¸€å¥
    if current_sentence is not None:
        sentences.append(current_sentence)
    
    return sentences


def get_local_model_path(model_name: str) -> str:
    """æŸ¥æ‰¾æœ¬åœ°æ¨¡å‹è·¯å¾„ã€‚"""
    possible_paths = [
        os.path.join(MODEL_DIR, f"faster-whisper-{model_name}"),
        os.path.join(MODEL_DIR, model_name),
        os.path.join(MODEL_DIR, f"Systran_faster-whisper-{model_name}"),
    ]
    for path in possible_paths:
        if os.path.exists(path) and os.path.isfile(os.path.join(path, "model.bin")):
            return path
    return None


@except_handler("Native Whisper processing error:")
def transcribe_audio_native(raw_audio_file, vocal_audio_file, start, end, WHISPER_LANGUAGE=None, device=None):
    """
    åŸç”Ÿ Faster-Whisper è½¬å†™ï¼ˆä¸ä½¿ç”¨ WhisperX VADï¼‰ã€‚
    
    ç‰¹ç‚¹ï¼š
    - ä¸åš pyannote VAD é¢„å¤„ç†ï¼ˆå‡å°‘æ¼æ®µï¼‰
    - ç›´æ¥è°ƒç”¨ faster-whisper è½¬å†™
    - è¾“å‡ºè¯çº§æ—¶é—´æˆ³
    - æ•ˆæœæ¥è¿‘ PotPlayer çš„è½¬å†™æ–¹å¼
    """
    # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–ä»é…ç½®è¯»å–
    if WHISPER_LANGUAGE is None:
        WHISPER_LANGUAGE = load_key("whisper.language")
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    rprint(f"ğŸš€ Starting Native Faster-Whisper using device: {device} ...")
    
    # -------------------------
    # è®¡ç®—è®¾ç½®
    # -------------------------
    if device == "cuda":
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        compute_type = "float16" if gpu_mem > 6 else "int8"
        rprint(f"[cyan]ğŸ® GPU memory:[/cyan] {gpu_mem:.2f} GB, [cyan]âš™ï¸ Compute type:[/cyan] {compute_type}")
    else:
        compute_type = "int8"
        rprint(f"[cyan]âš™ï¸ Compute type:[/cyan] {compute_type}")
    
    rprint(f"[green]â–¶ï¸ Processing segment {start:.2f}s to {end:.2f}s...[/green]")
    
    # -------------------------
    # åŠ è½½æ¨¡å‹
    # -------------------------
    if WHISPER_LANGUAGE == 'zh':
        model_name = "Huan69/Belle-whisper-large-v3-zh-punct-fasterwhisper"
        local_model = os.path.join(MODEL_DIR, "Belle-whisper-large-v3-zh-punct-fasterwhisper")
    else:
        model_name = load_key("whisper.model")
        local_model = get_local_model_path(model_name)
    
    if local_model and os.path.exists(local_model):
        rprint(f"[green]ğŸ“¥ Loading local model:[/green] {local_model}")
        model_path = local_model
    else:
        rprint(f"[green]ğŸ“¥ Loading model from HuggingFace:[/green] Systran/faster-whisper-{model_name}")
        model_path = f"Systran/faster-whisper-{model_name}"
    
    # ç›´æ¥åŠ è½½ Faster-Whisper æ¨¡å‹
    model = WhisperModel(
        model_path,
        device=device,
        compute_type=compute_type,
        download_root=MODEL_DIR
    )
    
    # -------------------------
    # è½¬å†™éŸ³é¢‘
    # -------------------------
    # æ³¨æ„ï¼šç›´æ¥ä¼ æ–‡ä»¶è·¯å¾„ç»™ faster-whisperï¼Œä¸è¦ç”¨ librosa è¯»å…¥ï¼
    # librosa é‡é‡‡æ ·ä¼šå¯¼è‡´è¯†åˆ«é”™è¯¯ï¼ˆå¦‚â€œäººç‰©æ··è¡Œâ€è€Œéâ€œç¥ä»æ ¹é¦™â€ï¼‰
    # faster-whisper å†…éƒ¨ä½¿ç”¨ PyAVï¼Œèƒ½æ­£ç¡®å¤„ç†éŸ³é¢‘
    
    transcribe_start_time = time.time()
    
    # è¯»å–é«˜çº§é…ç½®
    beam_size = load_key("whisper.beam_size") or 5
    best_of = load_key("whisper.best_of") or 5
    patience = load_key("whisper.patience") or 1.0
    
    whisper_language = None if WHISPER_LANGUAGE == 'auto' else WHISPER_LANGUAGE
    
    rprint(f"[cyan]ğŸ”§ Settings:[/cyan] beam_size={beam_size}, best_of={best_of}, patience={patience}")
    
    # -------------------------
    # è¯­è¨€åˆå§‹æç¤ºè¯ï¼ˆç±»ä¼¼ faster-whisper-xxl è‡ªåŠ¨æç¤ºï¼‰
    # ç”¨äºå¼•å¯¼æ¨¡å‹è¾“å‡ºæ­£ç¡®æ ‡ç‚¹å’Œæ±‰å­—/æ±‰å­—
    # -------------------------
    initial_prompt = get_language_prompt(whisper_language)
    
    if initial_prompt:
        rprint(f"[cyan]ğŸ“ Initial prompt:[/cyan] {initial_prompt}")
    
    # -------------------------
    # VAD å‚æ•°ï¼ˆåŸºäº RMS è‡ªåŠ¨é€‰æ‹©ï¼‰
    # -------------------------
    vad_parameters = select_vad_parameters(vocal_audio_file)
    rms_dbfs = vad_parameters.pop("_rms_dbfs", None)
    rprint(
        f"[cyan]ğŸ¤ VAD:[/cyan] RMS={rms_dbfs:.1f} dBFS, threshold={vad_parameters['threshold']}"
    )
    
    # ä½¿ç”¨è¯çº§æ—¶é—´æˆ³ + Silero VAD è½¬å†™
    # ç›´æ¥ä¼ æ–‡ä»¶è·¯å¾„ï¼ŒPyAV ä¼šæ­£ç¡®å¤„ç†éŸ³é¢‘
    segments_iter, info = model.transcribe(
        vocal_audio_file,  # ç›´æ¥ä¼ æ–‡ä»¶è·¯å¾„ï¼Œä¸è¦ç”¨ librosa
        language=whisper_language,
        beam_size=beam_size,
        best_of=best_of,
        patience=patience,
        word_timestamps=True,
        vad_filter=True,
        vad_parameters=vad_parameters,
        initial_prompt=initial_prompt,
        condition_on_previous_text=True,  # å¯ç”¨ä»¥è·å¾—æ›´å¥½æ ‡ç‚¹
        no_speech_threshold=0.6,
        log_prob_threshold=-1.0,
    )
    
    # è½¬ä¸ºåˆ—è¡¨å¹¶æ„å»ºç»“æœ
    segments = []
    for seg in segments_iter:
        # æ³¨æ„ï¼šä½¿ç”¨æ–‡ä»¶è·¯å¾„æ—¶ï¼Œæ—¶é—´æˆ³å·²æ˜¯ç»å¯¹æ—¶é—´
        # ä»…åœ¨éœ€è¦æ—¶æŒ‰ start/end è¿‡æ»¤
        if seg.start < start or seg.end > end:
            continue
            
        segment_data = {
            'start': seg.start,  # Already correct timestamps
            'end': seg.end,
            'text': seg.text.strip(),
            'words': []
        }
        
        # æ·»åŠ è¯çº§æ—¶é—´æˆ³
        if seg.words:
            for word in seg.words:
                if word.start < start or word.end > end:
                    continue
                segment_data['words'].append({
                    'word': word.word,
                    'start': word.start,
                    'end': word.end,
                    'probability': word.probability
                })
        
        segments.append(segment_data)
    
    transcribe_time = time.time() - transcribe_start_time
    rprint(f"[cyan]â±ï¸ Transcription time:[/cyan] {transcribe_time:.2f}s")
    rprint(f"[cyan]ğŸ“ Found {len(segments)} raw segments[/cyan]")
    
    # -------------------------
    # å¥å­çº§åå¤„ç†ï¼ˆç±»ä¼¼ --sentenceï¼‰
    # å·²å…³é—­åˆ†å¥ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åˆ†æ®µ
    # -------------------------
    sentences = segments
    rprint("[cyan]âœ‚ï¸ Sentence split disabled[/cyan]")
    
    # æ›´æ–°æ£€æµ‹åˆ°çš„è¯­è¨€
    detected_lang = info.language if hasattr(info, 'language') else WHISPER_LANGUAGE
    update_key("whisper.detected_language", detected_lang)
    
    # é‡Šæ”¾ GPU èµ„æº
    del model
    if device == "cuda":
        torch.cuda.empty_cache()
    
    return {'segments': sentences, 'language': detected_lang}
