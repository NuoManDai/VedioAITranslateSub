# VideoLingo Core æ¶æ„æ–‡æ¡£

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„å›¾](#ç³»ç»Ÿæ¶æ„å›¾)
3. [å¤„ç†æµç¨‹è¯¦è§£](#å¤„ç†æµç¨‹è¯¦è§£)
4. [UML å›¾](#uml-å›¾)
5. [æ¨¡å‹ä¸æŠ€æœ¯é€‰å‹](#æ¨¡å‹ä¸æŠ€æœ¯é€‰å‹)
6. [æ•°æ®æµå›¾](#æ•°æ®æµå›¾)

---

## é¡¹ç›®æ¦‚è¿°

VideoLingo æ˜¯ä¸€ä¸ªå®Œæ•´çš„è§†é¢‘æœ¬åœ°åŒ–å¤„ç†ç³»ç»Ÿï¼Œæ”¯æŒè§†é¢‘ä¸‹è½½ã€è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å­—å¹•åˆ†å‰²ã€ç¿»è¯‘ã€é…éŸ³ï¼ˆTTSï¼‰ã€éŸ³è§†é¢‘åˆæˆç­‰å…¨æµç¨‹è‡ªåŠ¨åŒ–å¤„ç†ã€‚

### æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

| æ¨¡å—ç¼–å· | æ–‡ä»¶å | åŠŸèƒ½æè¿° |
|---------|--------|---------|
| Step 1  | `_1_ytdlp.py` | è§†é¢‘ä¸‹è½½ï¼ˆyt-dlpï¼‰ |
| Step 2  | `_2_asr.py` | è¯­éŸ³è¯†åˆ«è½¬å½• |
| Step 3.1| `_3_1_split_nlp.py` | NLPå¥å­åˆ†å‰² |
| Step 3.2| `_3_2_split_meaning.py` | è¯­ä¹‰åˆ†å‰² |
| Step 4.1| `_4_1_summarize.py` | å†…å®¹æ‘˜è¦ä¸æœ¯è¯­æå– |
| Step 4.2| `_4_2_translate.py` | ç¿»è¯‘å¤„ç† |
| Step 5  | `_5_split_sub.py` | å­—å¹•åˆ†å‰²å¯¹é½ |
| Step 6  | `_6_gen_sub.py` | å­—å¹•æ–‡ä»¶ç”Ÿæˆ |
| Step 7  | `_7_sub_into_vid.py` | å­—å¹•çƒ§å½•åˆ°è§†é¢‘ |
| Step 8.1| `_8_1_audio_task.py` | é…éŸ³ä»»åŠ¡ç”Ÿæˆ |
| Step 8.2| `_8_2_dub_chunks.py` | é…éŸ³åˆ†å—å¤„ç† |
| Step 9  | `_9_refer_audio.py` | å‚è€ƒéŸ³é¢‘æå– |
| Step 10 | `_10_gen_audio.py` | TTS éŸ³é¢‘ç”Ÿæˆ |
| Step 11 | `_11_merge_audio.py` | éŸ³é¢‘åˆå¹¶ |
| Step 12 | `_12_dub_to_vid.py` | é…éŸ³åˆæˆåˆ°è§†é¢‘ |

---

## ç³»ç»Ÿæ¶æ„å›¾

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ è¾“å…¥å±‚"]
        YT[YouTube URL]
        Local[æœ¬åœ°è§†é¢‘]
    end

    subgraph Core["ğŸ”§ Core å¤„ç†å±‚"]
        subgraph ASR["ASR æ¨¡å—"]
            Demucs["Demucs äººå£°åˆ†ç¦»"]
            WhisperLocal["WhisperX Local<br/>(faster-whisper)"]
            WhisperCloud["WhisperX Cloud<br/>(302.ai)"]
            Demucs --> WhisperLocal
            Demucs --> WhisperCloud
        end
        
        subgraph NLP["NLP æ¨¡å—"]
            SpaCy["spaCy å¤šè¯­è¨€åˆ†è¯"]
            Split["æ ‡ç‚¹/é€—å·/è¿æ¥è¯åˆ†å‰²"]
            SyntaxTree["å¥æ³•æ ‘åˆ†æ"]
        end
        
        subgraph LLM["LLM æ¨¡å—"]
            SemanticSplit["GPT è¯­ä¹‰åˆ†å‰²"]
            TermExtract["æœ¯è¯­æå–ä¸æ‘˜è¦"]
            Translate["ç¿»è¯‘ (å¿ å®+è¾¾æ„)"]
            SubCompress["å­—å¹•å‹ç¼©ä¼˜åŒ–"]
        end
        
        subgraph TTS["TTS æ¨¡å—"]
            AzureTTS["Azure TTS"]
            OpenAITTS["OpenAI TTS"]
            EdgeTTS["Edge TTS (å…è´¹)"]
            GPTSOVITS["GPT-SoVITS (æœ¬åœ°)"]
            FishTTS["Fish TTS"]
            CosyVoice["CosyVoice2 / F5-TTS"]
        end
        
        subgraph Video["è§†é¢‘å¤„ç†æ¨¡å—"]
            FFmpeg["FFmpeg"]
            OpenCV["OpenCV"]
            Pydub["pydub"]
        end
    end

    subgraph External["â˜ï¸ å¤–éƒ¨ API"]
        LLMAPI["LLM API<br/>OpenAI / DeepSeek<br/>302.ai / SiliconFlow"]
        TTSAPI["TTS API<br/>Azure / OpenAI<br/>Fish / Edge"]
    end

    subgraph Output["ğŸ“¤ è¾“å‡ºå±‚"]
        SubVideo["output_sub.mp4<br/>å¸¦åŒè¯­å­—å¹•è§†é¢‘"]
        DubVideo["output_dub.mp4<br/>å¸¦é…éŸ³è§†é¢‘"]
        SRT["src.srt / trans.srt<br/>å­—å¹•æ–‡ä»¶"]
        DubAudio["dub.mp3<br/>é…éŸ³éŸ³é¢‘"]
    end

    Input --> ASR
    ASR --> NLP
    NLP --> LLM
    LLM --> TTS
    TTS --> Video
    Video --> Output
    
    LLM <--> LLMAPI
    TTS <--> TTSAPI
```

---

## å¤„ç†æµç¨‹è¯¦è§£

### å®Œæ•´å¤„ç†æµç¨‹å›¾

```mermaid
flowchart LR
    subgraph Stage1["é˜¶æ®µä¸€: é¢„å¤„ç†"]
        S1["Step 1<br/>è§†é¢‘ä¸‹è½½"]
        S2["Step 2<br/>è¯­éŸ³è¯†åˆ«"]
    end
    
    subgraph Stage2["é˜¶æ®µäºŒ: æ–‡æœ¬å¤„ç†"]
        S3_1["Step 3.1<br/>NLPåˆ†å‰²"]
        S3_2["Step 3.2<br/>è¯­ä¹‰åˆ†å‰²"]
        S4_1["Step 4.1<br/>æ‘˜è¦æå–"]
        S4_2["Step 4.2<br/>ç¿»è¯‘"]
    end
    
    subgraph Stage3["é˜¶æ®µä¸‰: å­—å¹•å¤„ç†"]
        S5["Step 5<br/>å­—å¹•åˆ†å‰²"]
        S6["Step 6<br/>ç”Ÿæˆå­—å¹•"]
        S7["Step 7<br/>çƒ§å½•å­—å¹•"]
    end
    
    subgraph Stage4["é˜¶æ®µå››: é…éŸ³å¤„ç†"]
        S8_1["Step 8.1<br/>é…éŸ³ä»»åŠ¡"]
        S8_2["Step 8.2<br/>é…éŸ³åˆ†å—"]
        S9["Step 9<br/>å‚è€ƒéŸ³é¢‘"]
        S10["Step 10<br/>TTSç”Ÿæˆ"]
        S11["Step 11<br/>éŸ³é¢‘åˆå¹¶"]
        S12["Step 12<br/>é…éŸ³åˆæˆ"]
    end

    S1 --> S2 --> S3_1 --> S3_2 --> S4_1 --> S4_2
    S4_2 --> S5 --> S6 --> S7
    S7 --> S8_1 --> S8_2 --> S9 --> S10 --> S11 --> S12
```

### Step 2: è¯­éŸ³è¯†åˆ«è¯¦ç»†æµç¨‹

```mermaid
flowchart TD
    A[è¾“å…¥: video.mp4] --> B[ffprobe æ£€æµ‹å£°é“æ•°]
    B --> C[FFmpeg æå–éŸ³é¢‘]
    C --> D[raw.mp3<br/>ä¿æŒåŸå§‹å£°é“ 16kHz]
    D --> E{å¯ç”¨äººå£°åˆ†ç¦»?}
    E -->|æ˜¯| F[Demucs åˆ†ç¦»]
    F --> G[vocal.mp3 äººå£°<br/>åŒå£°é“]
    F --> H[background.mp3 èƒŒæ™¯<br/>åŒå£°é“]
    E -->|å¦| I[ä½¿ç”¨ raw.mp3 ä½œä¸º vocal]
    
    D --> J{ASR æ¨¡å¼}
    J -->|æœ¬åœ°| K[faster-whisper è½¬å½•<br/>ä½¿ç”¨ raw.mp3]
    J -->|äº‘ç«¯| L[302.ai Whisper API<br/>ä½¿ç”¨ raw.mp3]
    
    K --> M[WhisperX æ—¶é—´æˆ³å¯¹é½]
    L --> M
    G --> M
    I --> M
    M -->|å¯¹é½ä½¿ç”¨ vocal| N[è¾“å‡º: cleaned_chunks.xlsx]
```

> **è¯´æ˜**: 
> - `raw.mp3` ä¿æŒä¸åŸå§‹è§†é¢‘ç›¸åŒçš„å£°é“æ•°ï¼ˆåŠ¨æ€æ£€æµ‹ï¼‰ï¼Œæ¯”ç‰¹ç‡ = 32k Ã— å£°é“æ•°
> - è½¬å½•é˜¶æ®µä½¿ç”¨ `raw.mp3`ï¼Œå¯¹é½é˜¶æ®µä½¿ç”¨ `vocal.mp3`ï¼ˆå¦‚æœå¯ç”¨äº† Demucsï¼‰
> - Demucs è¾“å‡ºå§‹ç»ˆä¸ºåŒå£°é“ï¼ˆæ¨¡å‹ç‰¹æ€§ï¼‰

### Step 3.1: æ–‡æœ¬ç²—åˆ‡åˆ†ï¼ˆNLP é¢„å¤„ç†ï¼‰

> **ğŸ“Œ æ³¨æ„ï¼šè¿™ä¸€æ­¥ä¸æ˜¯çœŸæ­£çš„"åˆ†å¥"ï¼Œè€Œæ˜¯æ–‡æœ¬ç²—åˆ‡åˆ†**
> 
> Step 3.1 çš„ç›®çš„æ˜¯å°† ASR è¾“å‡ºçš„é•¿æ–‡æœ¬æŒ‰**æ ‡ç‚¹ã€æ—¶é—´é—´éš”ã€è¿æ¥è¯**ç­‰è§„åˆ™è¿›è¡Œ**ç²—åˆ‡åˆ†**ï¼Œ
> ä¸ºåç»­çš„è¯­ä¹‰åˆ†å‰²æä¾›è¾ƒçŸ­çš„æ–‡æœ¬ç‰‡æ®µã€‚spaCy åœ¨è¿™é‡Œä¸»è¦ç”¨äºï¼š
> - **åˆ†è¯ï¼ˆtokenizeï¼‰**ï¼šè®¡ç®—æ–‡æœ¬é•¿åº¦
> - **ä¾å­˜åˆ†æ**ï¼šè¯†åˆ«è¿æ¥è¯ã€è¯æ ¹ç­‰è¯­æ³•ç»“æ„
> 
> **çœŸæ­£çš„æ™ºèƒ½åˆ†å¥åœ¨ Step 3.2 ç”± GPT å®Œæˆã€‚**

```mermaid
flowchart TD
    A[è¾“å…¥: cleaned_chunks.xlsx<br/>å­—ç¬¦çº§æ—¶é—´æˆ³æ•°æ®] --> B{å¯ç”¨æ—¶é—´é—´éš”åˆ‡åˆ†?<br/>time_gap_threshold > 0}
    
    B -->|æ˜¯| B1[æŒ‰æ—¶é—´é—´éš”é¢„åˆ‡åˆ†<br/>å•è¯æŒç»­æ—¶é—´ > é˜ˆå€¼ç§’]
    B -->|å¦| B2[è·³è¿‡æ—¶é—´åˆ‡åˆ†]
    
    B1 --> C[split_by_mark<br/>æŒ‰æ ‡ç‚¹ç²—åˆ‡åˆ†]
    B2 --> C
    
    C --> D[split_by_comma<br/>æŒ‰é€—å·åˆ‡åˆ†]
    D --> E[split_by_connector<br/>æŒ‰è¿æ¥è¯åˆ‡åˆ†<br/>spaCy ä¾å­˜åˆ†æ]
    E --> F{æ–‡æœ¬é•¿åº¦ >60 tokens?}
    F -->|æ˜¯| G[split_long_by_root<br/>åŠ¨æ€è§„åˆ’æŒ‰è¯æ ¹åˆ‡åˆ†]
    F -->|å¦| H[ä¿æŒåŸæ–‡æœ¬]
    G --> I[split_by_nlp.txt]
    H --> I
```

**spaCy åœ¨ Step 3.1 çš„ä½œç”¨**ï¼š

| åŠŸèƒ½ | ç”¨é€” | è¯´æ˜ |
|-----|------|------|
| åˆ†è¯ (tokenize) | è®¡ç®—æ–‡æœ¬é•¿åº¦ | åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ† |
| ä¾å­˜åˆ†æ (dep) | è¯†åˆ«è¿æ¥è¯ | `that`, `which`, `ã®ã§`, `ãŸã‚` ç­‰ |
| è¯æ€§æ ‡æ³¨ (pos) | è¯†åˆ«è¯æ ¹ | åŠ¨è¯ã€åè¯ç­‰ä½œä¸ºåˆ‡åˆ†ç‚¹ |
| å¥å­è¾¹ç•Œ (sents) | ç²—åˆ‡åˆ† | ä»…å¯¹æœ‰æ ‡ç‚¹çš„è¯­è¨€æœ‰æ•ˆ |

**æ—¶é—´é—´éš”åˆ‡åˆ†å‚æ•°**ï¼š

| å‚æ•° | é…ç½®é”® | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|-------|------|
| æ—¶é—´é—´éš”é˜ˆå€¼ | `time_gap_threshold` | ç©º (ä¸å¯ç”¨) | å•è¯æŒç»­æ—¶é—´è¶…è¿‡æ­¤å€¼(ç§’)æ—¶å¼ºåˆ¶åˆ‡åˆ†ï¼Œæ—¥è¯­æ¨è 1.0 |

> **æ—¥è¯­å¤„ç†ä¼˜åŒ–**ï¼šæ—¥è¯­å£è¯­é€šå¸¸æ²¡æœ‰æ˜æ˜¾æ ‡ç‚¹ï¼Œä½† ASR ä¼šåœ¨è‡ªç„¶åœé¡¿å¤„äº§ç”Ÿè¾ƒé•¿çš„å•è¯æŒç»­æ—¶é—´ã€‚
> è®¾ç½® `time_gap_threshold: 1.0` å¯ä»¥åˆ©ç”¨è¿™äº›åœé¡¿ç‚¹è¿›è¡Œåˆ‡åˆ†ã€‚

**æ–‡ä»¶æµ**ï¼š

```
cleaned_chunks.xlsx     â† ASR è¾“å‡ºï¼ˆå­—ç¬¦çº§æ—¶é—´æˆ³ï¼‰
    â†“ split_by_mark()       æŒ‰æ ‡ç‚¹/æ—¶é—´ç²—åˆ‡åˆ†
split_by_mark.txt (ä¸´æ—¶)
    â†“ split_by_comma_main() æŒ‰é€—å·åˆ‡åˆ†
split_by_comma.txt (ä¸´æ—¶)
    â†“ split_sentences_main() æŒ‰è¿æ¥è¯åˆ‡åˆ†
split_by_connector.txt (ä¸´æ—¶)
    â†“ split_long_by_root_main() æŒ‰è¯æ ¹åˆ‡åˆ†è¶…é•¿æ–‡æœ¬
split_by_nlp.txt        â† Step 3.1 æœ€ç»ˆè¾“å‡ºï¼ˆç²—åˆ‡åˆ†ç»“æœï¼‰
```

### Step 3.2: è¯­ä¹‰åˆ†å¥ï¼ˆGPT æ™ºèƒ½åˆ†å‰²ï¼‰

> **ğŸ“Œ è¿™ä¸€æ­¥æ‰æ˜¯çœŸæ­£çš„"åˆ†å¥"**
> 
> Step 3.2 ä½¿ç”¨ **GPT è¿›è¡Œè¯­ä¹‰ç†è§£**ï¼Œå°†ç²—åˆ‡åˆ†çš„æ–‡æœ¬ç‰‡æ®µè¿›ä¸€æ­¥åˆ†å‰²æˆ**è¯­ä¹‰å®Œæ•´çš„å¥å­**ã€‚
> spaCy åœ¨è¿™é‡Œåªç”¨äº **åˆ†è¯ï¼ˆtokenizeï¼‰** æ¥è®¡ç®—æ–‡æœ¬é•¿åº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨ GPTã€‚

```mermaid
flowchart TD
    A[è¾“å…¥: split_by_nlp.txt<br/>ç²—åˆ‡åˆ†æ–‡æœ¬] --> B[åŠ è½½ spaCy æ¨¡å‹]
    B --> C[éå†æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µ]
    C --> D[spaCy tokenize<br/>è®¡ç®— token æ•°é‡]
    
    D --> E{token æ•° > max_split_length?}
    E -->|â‰¤ 20 tokens| F[ä¿æŒåŸæ–‡æœ¬<br/>é•¿åº¦åˆé€‚æ— éœ€åˆ†å‰²]
    E -->|> 20 tokens| G[éœ€è¦ GPT åˆ†å¥]
    
    G --> H[è®¡ç®—åˆ†å‰²æ•°<br/>num_parts = ceil tokens/20]
    H --> I[è°ƒç”¨ GPT API]
    
    subgraph GPT["GPT è¯­ä¹‰åˆ†å¥"]
        I --> I1[Prompt: å°†å¥å­åˆ†æˆ N éƒ¨åˆ†]
        I1 --> I2[GPT ç†è§£è¯­ä¹‰]
        I2 --> I3[è¿”å›: å¥å­1 âˆ¥ å¥å­2 âˆ¥ å¥å­3]
    end
    
    I3 --> J[find_split_positions<br/>åœ¨åŸæ–‡ä¸­å®šä½åˆ†å‰²ç‚¹]
    
    subgraph Align["åˆ†å‰²ç‚¹å¯¹é½"]
        J --> J1[è®¡ç®— GPT ç»“æœä¸åŸæ–‡çš„ç›¸ä¼¼åº¦]
        J1 --> J2{ç›¸ä¼¼åº¦ > 0.9?}
        J2 -->|æ˜¯| J3[è®°å½•åˆ†å‰²ä½ç½®]
        J2 -->|å¦| J4[Warning + å°½åŠ›åŒ¹é…]
    end
    
    J3 --> K[åœ¨åˆ†å‰²ç‚¹æ’å…¥æ¢è¡Œ]
    J4 --> K
    F --> L[split_by_meaning.txt]
    K --> L
    
    L --> M{è¿˜æœ‰è¶…é•¿å¥å­?}
    M -->|æ˜¯| N[é€’å½’å¤„ç†<br/>æœ€å¤š 3 æ¬¡]
    M -->|å¦| O[å®Œæˆ]
    N --> C
```

**spaCy åœ¨ Step 3.2 çš„ä½œç”¨**ï¼š

| åŠŸèƒ½ | ç”¨é€” |
|-----|------|
| **åˆ†è¯ (tokenize)** | è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼Œåˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼éœ€è¦ GPT åˆ†å¥ |

> **æ³¨æ„**ï¼šStep 3.2 ä¸­ spaCy **ä¸åšåˆ†å¥**ï¼Œåˆ†å¥å®Œå…¨ç”± GPT å®Œæˆã€‚

**è¯­ä¹‰åˆ†å¥å…³é”®å‚æ•°**:

| å‚æ•° | é…ç½®é”® | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|-------|------|
| æœ€å¤§åˆ†å‰²é•¿åº¦ | `max_split_length` | æ—¥è¯­12 / å…¶ä»–20 | è¶…è¿‡æ­¤ token æ•°è§¦å‘ GPT åˆ†å¥ |
| æ—¶é—´é—´éš”é˜ˆå€¼ | `time_gap_threshold` | ç©º (ä¸å¯ç”¨) | Step 3.1 ä¸­æŒ‰æ—¶é—´åˆ‡åˆ†çš„é˜ˆå€¼(ç§’) |
| å¹¶å‘æ•° | `max_workers` | 4 | GPT è¯·æ±‚å¹¶å‘æ•° |
| ç›¸ä¼¼åº¦é˜ˆå€¼ | - | 0.9 | åˆ†å‰²ç‚¹å®šä½çš„æœ€å°ç›¸ä¼¼åº¦ |
| æœ€å¤§é‡è¯•æ¬¡æ•° | - | 3 | é€’å½’å¤„ç†è¶…é•¿å¥å­çš„æ¬¡æ•° |

**è¯­è¨€æ¨¡å‹é€‰æ‹©é€»è¾‘**:

```python
# init_nlp() è¯­è¨€é€‰æ‹© - ç”¨äºåˆ†è¯
user_language = load_key("whisper.language")      # ç”¨æˆ·è®¾ç½®çš„è¯­è¨€
detected_language = load_key("whisper.detected_language")  # è‡ªåŠ¨æ£€æµ‹çš„è¯­è¨€
language = user_language if user_language else detected_language

# æ˜ å°„åˆ° spaCy æ¨¡å‹ï¼ˆç”¨äºåˆ†è¯ï¼Œä¸æ˜¯åˆ†å¥ï¼‰
SPACY_MODEL_MAP = {
    "ja": "ja_core_news_md",
    "en": "en_core_web_md", 
    "zh": "zh_core_web_md",
    ...
}
```

**GPT åˆ†å‰² Prompt ç¤ºä¾‹**:

```
è¯·å°†ä»¥ä¸‹å¥å­åˆ†æˆ 3 éƒ¨åˆ†ï¼Œç”¨ || åˆ†éš”:
"é«˜ãƒ¬ãƒ™ãƒ«ã®è­¦æˆ’éš è”½ã‚’ä½¿ã†ã“ã¨ã¯ãƒ¨ã‚¬ãƒ©ã‚¹ã®ã‚«ãƒ¡ãƒ©ã‚’é€šã—ã¦è¦‹ã¦ã„ãŸã®ã§ãªãŠå‰ãŒç‹å¥³ã«ã¤ãã¾ã¨ã£ã¦ã„ã‚‹ã¨çŸ¥ã‚Šã‚µãƒ©ãƒ ã®é­”çœ¼ã«ä¼¼ã›ãŸä»•çµ„ã¿ã‚’ä½œã‚‰ã›ãŸã®ã "

GPT è¿”å›:
"é«˜ãƒ¬ãƒ™ãƒ«ã®è­¦æˆ’éš è”½ã‚’ä½¿ã†ã“ã¨ã¯ãƒ¨ã‚¬ãƒ©ã‚¹ã®ã‚«ãƒ¡ãƒ©ã‚’é€šã—ã¦è¦‹ã¦ã„ãŸã®ã§||ãªãŠå‰ãŒç‹å¥³ã«ã¤ãã¾ã¨ã£ã¦ã„ã‚‹ã¨çŸ¥ã‚Š||ã‚µãƒ©ãƒ ã®é­”çœ¼ã«ä¼¼ã›ãŸä»•çµ„ã¿ã‚’ä½œã‚‰ã›ãŸã®ã "
```

### Step 4.2: ç¿»è¯‘åŒæ­¥éª¤æµç¨‹

```mermaid
flowchart TD
    A[è¾“å…¥æ–‡æœ¬] --> B[Step 1: å¿ å®ç¿»è¯‘]
    B --> C{å¯ç”¨è¾¾æ„ç¿»è¯‘?}
    C -->|æ˜¯| D[Step 2: è¾¾æ„ç¿»è¯‘]
    C -->|å¦| E[è¾“å‡ºç¿»è¯‘ç»“æœ]
    D --> E
    
    subgraph Step1["å¿ å®ç¿»è¯‘ (Faithfulness)"]
        B1["ç›´è¯‘åŸæ–‡"]
        B2["ä¿æŒåŸæ„"]
        B3["å‚è€ƒæœ¯è¯­è¡¨"]
    end
    
    subgraph Step2["è¾¾æ„ç¿»è¯‘ (Expressiveness)"]
        D1["æ¶¦è‰²è¡¨è¾¾"]
        D2["ä¼˜åŒ–è¯­åº"]
        D3["é€‚åº”ç›®æ ‡è¯­è¨€"]
    end
    
    B --> Step1
    D --> Step2
```

---

## UML å›¾

### å¤„ç†æµç¨‹åºåˆ—å›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant YT as yt-dlp
    participant Demucs as Demucs
    participant Whisper as WhisperX
    participant SpaCy as spaCy
    participant LLM as LLM API
    participant TTS as TTS API
    participant FFmpeg as FFmpeg

    User->>YT: 1. ä¸‹è½½è§†é¢‘
    YT-->>User: video.mp4
    
    User->>Demucs: 2. äººå£°åˆ†ç¦»
    Demucs-->>User: vocal.mp3 + background.mp3
    
    User->>Whisper: 3. è¯­éŸ³è¯†åˆ« (raw.mp3)
    Whisper-->>User: è½¬å½•æ–‡æœ¬
    User->>Whisper: æ—¶é—´æˆ³å¯¹é½ (vocal.mp3)
    Whisper-->>User: cleaned_chunks.xlsx
    
    rect rgb(200, 230, 255)
        Note over SpaCy: Step 3.1 NLP åˆ†å¥
        User->>SpaCy: 4a. æ ‡ç‚¹/æ—¶é—´åˆ†å¥
        SpaCy-->>User: split_by_mark.txt
        User->>SpaCy: 4b. é€—å·åˆ†å‰²
        SpaCy-->>User: split_by_comma.txt
        User->>SpaCy: 4c. è¿æ¥è¯åˆ†å‰²
        SpaCy-->>User: split_by_connector.txt
        User->>SpaCy: 4d. é•¿å¥æŒ‰è¯æ ¹åˆ†å‰²
        SpaCy-->>User: split_by_nlp.txt
    end
    
    rect rgb(255, 230, 200)
        Note over LLM: Step 3.2 è¯­ä¹‰åˆ†å‰²
        User->>SpaCy: 5a. åŠ è½½è¯­è¨€æ¨¡å‹
        SpaCy-->>User: nlp (ja/en/zh...)
        User->>SpaCy: 5b. Tokenize è®¡ç®—é•¿åº¦
        SpaCy-->>User: token æ•°é‡
        User->>LLM: 5c. GPT åˆ†å‰²è¶…é•¿å¥
        LLM-->>User: åˆ†å‰²ç‚¹ (||)
        User->>User: 5d. å®šä½å¹¶åº”ç”¨åˆ†å‰²
        User-->>User: split_by_meaning.txt
    end
    
    User->>LLM: 6. æœ¯è¯­æå–
    LLM-->>User: terminology.json
    
    User->>LLM: 7. ç¿»è¯‘
    LLM-->>User: ç¿»è¯‘ç»“æœ
    
    User->>LLM: 8. å­—å¹•å‹ç¼©
    LLM-->>User: é€‚é…é•¿åº¦çš„å­—å¹•
    
    User->>TTS: 9. ç”Ÿæˆé…éŸ³
    TTS-->>User: éŸ³é¢‘ç‰‡æ®µ
    
    User->>FFmpeg: 10. éŸ³è§†é¢‘åˆæˆ
    FFmpeg-->>User: output_dub.mp4
```

### æ¨¡å—ç±»å›¾

```mermaid
classDiagram
    class Core {
        +_1_ytdlp.py
        +_2_asr.py
        +_3_1_split_nlp.py
        +_3_2_split_meaning.py
        +_4_1_summarize.py
        +_4_2_translate.py
        +_5_split_sub.py
        +_6_gen_sub.py
        +_7_sub_into_vid.py
        +_8_1_audio_task.py
        +_8_2_dub_chunks.py
        +_9_refer_audio.py
        +_10_gen_audio.py
        +_11_merge_audio.py
        +_12_dub_to_vid.py
    }
    
    class ASRBackend {
        +whisperX_local.py
        +whisperX_302.py
        +elevenlabs_asr.py
        +demucs_vl.py
        +audio_preprocess.py
        +transcribe_audio()
    }
    
    class SpacyUtils {
        +load_nlp_model()
        +split_by_mark()
        +split_by_comma()
        +split_by_connector()
        +split_long_by_root()
    }
    
    class TTSBackend {
        +tts_main.py
        +azure_tts.py
        +openai_tts.py
        +edge_tts.py
        +gpt_sovits_tts.py
        +fish_tts.py
        +sf_cosyvoice2.py
        +generate_audio()
    }
    
    class Utils {
        +ask_gpt()
        +load_key()
        +check_file_exists()
        +except_handler()
        +rprint()
    }
    
    Core --> ASRBackend : uses
    Core --> SpacyUtils : uses
    Core --> TTSBackend : uses
    Core --> Utils : uses
```

### çŠ¶æ€æœºå›¾

```mermaid
stateDiagram-v2
    [*] --> Download: å¼€å§‹å¤„ç†
    
    Download --> ASR: ä¸‹è½½å®Œæˆ
    Download --> Error: ä¸‹è½½å¤±è´¥
    
    ASR --> NLPSplit: ASRå®Œæˆ
    ASR --> Error: ASRå¤±è´¥
    
    NLPSplit --> LLMSplit: NLPåˆ†å‰²å®Œæˆ
    NLPSplit --> Error: åˆ†å‰²å¤±è´¥
    
    LLMSplit --> Summarize: è¯­ä¹‰åˆ†å‰²å®Œæˆ
    
    Summarize --> Translate: æœ¯è¯­æå–å®Œæˆ
    
    Translate --> Pause: pause_before_translate=true
    Translate --> SubSplit: ç¿»è¯‘å®Œæˆ
    
    Pause --> SubSplit: ç”¨æˆ·ç¡®è®¤ç»§ç»­
    
    SubSplit --> GenSub: å­—å¹•åˆ†å‰²å®Œæˆ
    
    GenSub --> BurnSub: å­—å¹•ç”Ÿæˆå®Œæˆ
    
    BurnSub --> AudioTask: å­—å¹•çƒ§å½•å®Œæˆ
    
    AudioTask --> DubChunks: ä»»åŠ¡ç”Ÿæˆå®Œæˆ
    
    DubChunks --> ReferAudio: åˆ†å—å®Œæˆ
    
    ReferAudio --> TTSGen: å‚è€ƒéŸ³é¢‘æå–å®Œæˆ
    
    TTSGen --> MergeAudio: TTSç”Ÿæˆå®Œæˆ
    TTSGen --> Error: TTSå¤±è´¥
    
    MergeAudio --> DubVid: éŸ³é¢‘åˆå¹¶å®Œæˆ
    
    DubVid --> [*]: å¤„ç†å®Œæˆ
    
    Error --> [*]: å¤„ç†ç»ˆæ­¢
```

---

## æ¨¡å‹ä¸æŠ€æœ¯é€‰å‹

### ASR æ¨¡å‹å¯¹æ¯”

```mermaid
graph LR
    subgraph Local["æœ¬åœ°æ¨¡å‹"]
        A["faster-whisper-large-v3<br/>é«˜ç²¾åº¦"]
        B["faster-whisper-large-v3-turbo<br/>é€Ÿåº¦å¿«"]
        C["Belle-whisper-large-v3-zh<br/>ä¸­æ–‡ä¼˜åŒ–"]
    end
    
    subgraph Cloud["äº‘ç«¯ API"]
        D["302.ai Whisper<br/>æ— éœ€ GPU"]
        E["ElevenLabs ASR<br/>é«˜è´¨é‡"]
    end
    
    subgraph Alignment["æ—¶é—´æˆ³å¯¹é½"]
        F["WhisperX<br/>å•è¯çº§å¯¹é½"]
    end
    
    A --> F
    B --> F
    C --> F
    D --> F
    E --> F
```

### TTS åŠŸèƒ½æ”¯æŒè¡¨

| å¼•æ“ | è¯­è¨€æ”¯æŒ | å£°éŸ³å…‹éš† | æˆæœ¬ | API æ¥æº |
|-----|---------|---------|------|---------|
| **Azure TTS** | 100+ | âŒ | ä»˜è´¹ | 302.ai |
| **OpenAI TTS** | å¤šè¯­è¨€ | âŒ | ä»˜è´¹ | 302.ai |
| **Edge TTS** | å¤šè¯­è¨€ | âŒ | å…è´¹ | å¾®è½¯ |
| **GPT-SoVITS** | å¤šè¯­è¨€ | âœ… | æœ¬åœ°éƒ¨ç½² | æœ¬åœ° |
| **Fish TTS** | ä¸­/è‹± | âœ… | ä»˜è´¹ | 302.ai / SiliconFlow |
| **CosyVoice2** | ä¸­/è‹± | âœ… | ä»˜è´¹ | SiliconFlow |
| **F5-TTS** | å¤šè¯­è¨€ | âœ… | ä»˜è´¹ | 302.ai |

### NLP æ¨¡å‹æ”¯æŒ

| è¯­è¨€ | spaCy æ¨¡å‹ | ç”¨é€” |
|-----|-----------|------|
| English | `en_core_web_md` | åˆ†è¯ã€å¥æ³•åˆ†æ |
| Chinese | `zh_core_web_md` | ä¸­æ–‡åˆ†è¯ |
| Japanese | `ja_core_news_md` | æ—¥æ–‡åˆ†è¯ |
| German | `de_core_news_md` | å¾·æ–‡åˆ†è¯ |
| French | `fr_core_news_md` | æ³•æ–‡åˆ†è¯ |
| Spanish | `es_core_news_md` | è¥¿ç­ç‰™æ–‡åˆ†è¯ |

---

## æ•°æ®æµå›¾

### æ–‡ä»¶æ•°æ®æµ

```mermaid
flowchart TD
    subgraph Input["è¾“å…¥"]
        V["video.mp4<br/>æˆ– YouTube URL"]
    end
    
    subgraph Audio["output/audio/"]
        A1["raw.mp3"]
        A2["vocal.mp3"]
        A3["background.mp3"]
        A4["refers/*.wav"]
        A5["segs/*.wav"]
        A6["audio_task.xlsx"]
    end
    
    subgraph Log["output/log/"]
        L1["cleaned_chunks.xlsx"]
        L2["split_by_nlp.txt"]
        L3["split_by_meaning.txt"]
        L4["translation.xlsx"]
        L5["split_sub.xlsx"]
    end
    
    subgraph GPTLog["output/gpt_log/"]
        G1["terminology.json"]
        G2["summary.json"]
    end
    
    subgraph Final["output/"]
        F1["src.srt"]
        F2["trans.srt"]
        F3["src_trans.srt"]
        F4["dub.mp3"]
        F5["output_sub.mp4"]
        F6["output_dub.mp4"]
    end
    
    V -->|Step 2| A1
    A1 -->|Demucs| A2
    A1 -->|Demucs| A3
    A2 -->|WhisperX| L1
    L1 -->|Step 3.1| L2
    L2 -->|Step 3.2| L3
    L3 -->|Step 4.1| G1
    L3 -->|Step 4.1| G2
    L3 -->|Step 4.2| L4
    L4 -->|Step 5| L5
    L5 -->|Step 6| F1
    L5 -->|Step 6| F2
    L5 -->|Step 6| F3
    F2 -->|Step 7| F5
    L5 -->|Step 8| A6
    A2 -->|Step 9| A4
    A6 -->|Step 10| A5
    A5 -->|Step 11| F4
    F4 -->|Step 12| F6
    A3 -->|Step 12| F6
```

### é…ç½®å‚æ•°å…³ç³»å›¾

```mermaid
mindmap
  root((config.yaml))
    APIé…ç½®
      api.key
      api.base_url
      api.model
    è¯­è¨€é…ç½®
      source_language
      target_language
      whisper.method
      whisper.language
    å¤„ç†å‚æ•°
      max_workers
      max_split_length
      time_gap_threshold
      summary_length
      reflect_translate
    TTSé…ç½®
      tts_method
      speed_factor.accept
      speed_factor.min
      voice_character
    å­—å¹•é…ç½®
      subtitle.max_length
      subtitle.target_multiplier
      burn_subtitles
    ç½‘ç»œé…ç½®
      hf_mirror
      http_proxy
```

---

## æ€»ç»“

VideoLingo æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–è®¾è®¡çš„è§†é¢‘æœ¬åœ°åŒ–ç³»ç»Ÿï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

```mermaid
mindmap
  root((VideoLingo))
    æ¶æ„ç‰¹ç‚¹
      æµæ°´çº¿æ¶æ„
        12ä¸ªç‹¬ç«‹æ­¥éª¤
        ä¸­é—´æ–‡ä»¶äº§å‡º
        æ–­ç‚¹ç»­ä¼ æ”¯æŒ
      å¤šæ¨¡å‹æ”¯æŒ
        ASRå¼•æ“åˆ‡æ¢
        TTSå¼•æ“åˆ‡æ¢
        LLMæ¨¡å‹åˆ‡æ¢
    æ ¸å¿ƒèƒ½åŠ›
      æ™ºèƒ½å¤„ç†
        LLMè¯­ä¹‰åˆ†å‰²
        åŒæ­¥éª¤ç¿»è¯‘
        æœ¯è¯­ä¸€è‡´æ€§
      æ€§èƒ½ä¼˜åŒ–
        å¹¶è¡Œå¤„ç†
        GPUåŠ é€Ÿ
        ç¼“å­˜æœºåˆ¶
    é…ç½®çµæ´»
      config.yaml
      APIå¯†é’¥ç®¡ç†
      å¤šè¯­è¨€æ”¯æŒ
```

### æŠ€æœ¯æ ˆæ€»è§ˆ

```mermaid
pie title æŠ€æœ¯æ ˆç»„æˆ
    "Python Core" : 40
    "AI/ML Models" : 25
    "FFmpeg/Audio" : 15
    "FastAPI/React" : 12
    "External APIs" : 8
```
