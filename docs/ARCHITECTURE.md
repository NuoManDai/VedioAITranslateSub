# VideoLingo Core æ¶æ„æ–‡æ¡£

## ğŸ“‹ ç›®å½•

1. [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
2. [ç³»ç»Ÿæ¶æ„å›¾](#ç³»ç»Ÿæ¶æ„å›¾)
3. [å¤„ç†æµç¨‹è¯¦è§£](#å¤„ç†æµç¨‹è¯¦è§£)
4. [UML å›¾](#uml-å›¾)
5. [æ¨¡å‹ä¸æŠ€æœ¯é€‰å‹](#æ¨¡å‹ä¸æŠ€æœ¯é€‰å‹)
6. [æ•°æ®æµå›¾](#æ•°æ®æµå›¾)

---

## é¡¹ç›®æ¦‚è¿°

VideoLingo æ˜¯ä¸€ä¸ªå®Œæ•´çš„è§†é¢‘æœ¬åœ°åŒ–å¤„ç†ç³»ç»Ÿï¼Œæ”¯æŒè§†é¢‘ä¸‹è½½ã€è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ã€å­—å¹•åˆ†å‰²ã€ç¿»è¯‘ã€é…éŸ³ï¼ˆTTSï¼‰ã€éŸ³è§†é¢‘åˆæˆç­‰å…¨æµç¨‹è‡ªåŠ¨åŒ–å¤„ç†ã€‚

### æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

| æ¨¡å—ç¼–å· | æ–‡ä»¶å | åŠŸèƒ½æè¿° |
|---------|--------|---------|
| Step 1  | `_1_ytdlp.py` | è§†é¢‘ä¸‹è½½ï¼ˆyt-dlpï¼‰ |
| Step 2  | `_2_asr.py` | è¯­éŸ³è¯†åˆ«è½¬å½• |
| Step 3.1| `_3_1_split_nlp.py` | NLPå¥å­åˆ†å‰² |
| Step 3.2| `_3_2_split_meaning.py` | è¯­ä¹‰åˆ†å‰² |
| Step 4.1| `_4_1_summarize.py` | å†…å®¹æ‘˜è¦ä¸æœ¯è¯­æå– |
| Step 4.2| `_4_2_translate.py` | ç¿»è¯‘å¤„ç† |
| Step 5  | `_5_split_sub.py` | å­—å¹•åˆ†å‰²å¯¹é½ |
| Step 6  | `_6_gen_sub.py` | å­—å¹•æ–‡ä»¶ç”Ÿæˆ |
| Step 7  | `_7_sub_into_vid.py` | å­—å¹•çƒ§å½•åˆ°è§†é¢‘ |
| Step 8.1| `_8_1_audio_task.py` | é…éŸ³ä»»åŠ¡ç”Ÿæˆ |
| Step 8.2| `_8_2_dub_chunks.py` | é…éŸ³åˆ†å—å¤„ç† |
| Step 9  | `_9_refer_audio.py` | å‚è€ƒéŸ³é¢‘æå– |
| Step 10 | `_10_gen_audio.py` | TTS éŸ³é¢‘ç”Ÿæˆ |
| Step 11 | `_11_merge_audio.py` | éŸ³é¢‘åˆå¹¶ |
| Step 12 | `_12_dub_to_vid.py` | é…éŸ³åˆæˆåˆ°è§†é¢‘ |

---

## ç³»ç»Ÿæ¶æ„å›¾

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ è¾“å…¥å±‚"]
        YT[YouTube URL]
        Local[æœ¬åœ°è§†é¢‘]
    end

    subgraph Core["ğŸ”§ Core å¤„ç†å±‚"]
        subgraph ASR["ASR æ¨¡å—"]
            Demucs["Demucs äººå£°åˆ†ç¦»"]
            WhisperLocal["WhisperX Local<br/>(faster-whisper)"]
            WhisperCloud["WhisperX Cloud<br/>(302.ai)"]
            Demucs --> WhisperLocal
            Demucs --> WhisperCloud
        end
        
        subgraph NLP["NLP æ¨¡å—"]
            SpaCy["spaCy å¤šè¯­è¨€åˆ†è¯"]
            Split["æ ‡ç‚¹/é€—å·/è¿æ¥è¯åˆ†å‰²"]
            SyntaxTree["å¥æ³•æ ‘åˆ†æ"]
        end
        
        subgraph LLM["LLM æ¨¡å—"]
            SemanticSplit["GPT è¯­ä¹‰åˆ†å‰²"]
            TermExtract["æœ¯è¯­æå–ä¸æ‘˜è¦"]
            Translate["ç¿»è¯‘ (å¿ å®+è¾¾æ„)"]
            SubCompress["å­—å¹•å‹ç¼©ä¼˜åŒ–"]
        end
        
        subgraph TTS["TTS æ¨¡å—"]
            AzureTTS["Azure TTS"]
            OpenAITTS["OpenAI TTS"]
            EdgeTTS["Edge TTS (å…è´¹)"]
            GPTSOVITS["GPT-SoVITS (æœ¬åœ°)"]
            FishTTS["Fish TTS"]
            CosyVoice["CosyVoice2 / F5-TTS"]
        end
        
        subgraph Video["è§†é¢‘å¤„ç†æ¨¡å—"]
            FFmpeg["FFmpeg"]
            OpenCV["OpenCV"]
            Pydub["pydub"]
        end
    end

    subgraph External["â˜ï¸ å¤–éƒ¨ API"]
        LLMAPI["LLM API<br/>OpenAI / DeepSeek<br/>302.ai / SiliconFlow"]
        TTSAPI["TTS API<br/>Azure / OpenAI<br/>Fish / Edge"]
    end

    subgraph Output["ğŸ“¤ è¾“å‡ºå±‚"]
        SubVideo["output_sub.mp4<br/>å¸¦åŒè¯­å­—å¹•è§†é¢‘"]
        DubVideo["output_dub.mp4<br/>å¸¦é…éŸ³è§†é¢‘"]
        SRT["src.srt / trans.srt<br/>å­—å¹•æ–‡ä»¶"]
        DubAudio["dub.mp3<br/>é…éŸ³éŸ³é¢‘"]
    end

    Input --> ASR
    ASR --> NLP
    NLP --> LLM
    LLM --> TTS
    TTS --> Video
    Video --> Output
    
    LLM <--> LLMAPI
    TTS <--> TTSAPI
```

---

## å¤„ç†æµç¨‹è¯¦è§£

### å®Œæ•´å¤„ç†æµç¨‹å›¾

```mermaid
flowchart LR
    subgraph Stage1["é˜¶æ®µä¸€: é¢„å¤„ç†"]
        S1["Step 1<br/>è§†é¢‘ä¸‹è½½"]
        S2["Step 2<br/>è¯­éŸ³è¯†åˆ«"]
    end
    
    subgraph Stage2["é˜¶æ®µäºŒ: æ–‡æœ¬å¤„ç†"]
        S3_1["Step 3.1<br/>NLPåˆ†å‰²"]
        S3_2["Step 3.2<br/>è¯­ä¹‰åˆ†å‰²"]
        S4_1["Step 4.1<br/>æ‘˜è¦æå–"]
        S4_2["Step 4.2<br/>ç¿»è¯‘"]
    end
    
    subgraph Stage3["é˜¶æ®µä¸‰: å­—å¹•å¤„ç†"]
        S5["Step 5<br/>å­—å¹•åˆ†å‰²"]
        S6["Step 6<br/>ç”Ÿæˆå­—å¹•"]
        S7["Step 7<br/>çƒ§å½•å­—å¹•"]
    end
    
    subgraph Stage4["é˜¶æ®µå››: é…éŸ³å¤„ç†"]
        S8_1["Step 8.1<br/>é…éŸ³ä»»åŠ¡"]
        S8_2["Step 8.2<br/>é…éŸ³åˆ†å—"]
        S9["Step 9<br/>å‚è€ƒéŸ³é¢‘"]
        S10["Step 10<br/>TTSç”Ÿæˆ"]
        S11["Step 11<br/>éŸ³é¢‘åˆå¹¶"]
        S12["Step 12<br/>é…éŸ³åˆæˆ"]
    end

    S1 --> S2 --> S3_1 --> S3_2 --> S4_1 --> S4_2
    S4_2 --> S5 --> S6 --> S7
    S7 --> S8_1 --> S8_2 --> S9 --> S10 --> S11 --> S12
```

### Step 2: è¯­éŸ³è¯†åˆ«è¯¦ç»†æµç¨‹

```mermaid
flowchart TD
    A[è¾“å…¥: video.mp4] --> B[ffprobe æ£€æµ‹å£°é“æ•°]
    B --> C[FFmpeg æå–éŸ³é¢‘]
    C --> D[raw.mp3<br/>ä¿æŒåŸå§‹å£°é“ 16kHz]
    D --> E{å¯ç”¨äººå£°åˆ†ç¦»?}
    E -->|æ˜¯| F[Demucs åˆ†ç¦»]
    F --> G[vocal.mp3 äººå£°<br/>åŒå£°é“]
    F --> H[background.mp3 èƒŒæ™¯<br/>åŒå£°é“]
    E -->|å¦| I[ä½¿ç”¨ raw.mp3 ä½œä¸º vocal]
    
    D --> J{ASR æ¨¡å¼}
    J -->|æœ¬åœ°| K[faster-whisper è½¬å½•<br/>ä½¿ç”¨ raw.mp3]
    J -->|äº‘ç«¯| L[302.ai Whisper API<br/>ä½¿ç”¨ raw.mp3]
    
    K --> M[WhisperX æ—¶é—´æˆ³å¯¹é½]
    L --> M
    G --> M
    I --> M
    M -->|å¯¹é½ä½¿ç”¨ vocal| N{å¯ç”¨è¯´è¯äººåˆ†ç¦»?}
    
    N -->|æ˜¯| O[pyannote.audio åˆ†ç¦»]
    N -->|å¦| P[speaker = NaN]
    
    O --> Q[whisperx.assign_word_speakers<br/>åˆ†é…è¯´è¯äººåˆ°æ¯ä¸ªè¯]
    Q --> R[è¾“å‡º: cleaned_chunks.xlsx<br/>åŒ…å« speaker åˆ—]
    P --> R
```

> **è¯´æ˜**: 
> - `raw.mp3` ä¿æŒä¸åŸå§‹è§†é¢‘ç›¸åŒçš„å£°é“æ•°ï¼ˆåŠ¨æ€æ£€æµ‹ï¼‰ï¼Œæ¯”ç‰¹ç‡ = 32k Ã— å£°é“æ•°
> - è½¬å½•é˜¶æ®µä½¿ç”¨ `raw.mp3`ï¼Œå¯¹é½é˜¶æ®µä½¿ç”¨ `vocal.mp3`ï¼ˆå¦‚æœå¯ç”¨äº† Demucsï¼‰
> - Demucs è¾“å‡ºå§‹ç»ˆä¸ºåŒå£°é“ï¼ˆæ¨¡å‹ç‰¹æ€§ï¼‰

### Step 2 è¡¥å……ï¼šè¯´è¯äººåˆ†ç¦»ï¼ˆSpeaker Diarizationï¼‰

> **ğŸ“Œ è¯´è¯äººåˆ†ç¦»çš„ä½œç”¨**
>
> å½“è§†é¢‘ä¸­æœ‰å¤šä¸ªè¯´è¯äººæ—¶ï¼Œè¯´è¯äººåˆ†ç¦»å¯ä»¥è¯†åˆ«å‡º"è°åœ¨ä»€ä¹ˆæ—¶å€™è¯´è¯"ï¼Œ
> ä¸ºåç»­çš„ç¿»è¯‘å’Œé…éŸ³æä¾›æ›´ç²¾ç¡®çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

#### å®Œæ•´å¤„ç†æµç¨‹

```mermaid
flowchart TD
    A[raw.mp3<br/>åŸå§‹éŸ³é¢‘ 16kHz] --> B{speaker_diarization<br/>é…ç½®å¼€å¯?}
    
    B -->|å¦| Z1[è·³è¿‡è¯´è¯äººåˆ†ç¦»<br/>speaker = NaN]
    B -->|æ˜¯| C{hf_token<br/>æœ‰æ•ˆ?}
    
    C -->|å¦| Z2[è­¦å‘Š: æ— æœ‰æ•ˆ Token<br/>è·³è¿‡è¯´è¯äººåˆ†ç¦»]
    C -->|æ˜¯| D[åŠ è½½ pyannote Pipeline]
    
    subgraph LoadModel["æ¨¡å‹åŠ è½½é˜¶æ®µ"]
        D --> D1[æ£€æŸ¥ HF_ENDPOINT<br/>è®¾ç½®é•œåƒåœ°å€]
        D1 --> D2[Pipeline.from_pretrained<br/>pyannote/speaker-diarization-3.1]
        D2 --> D3[æ¨¡å‹ç§»åŠ¨åˆ° GPU/CPU]
    end
    
    D3 --> E[éŸ³é¢‘é¢„å¤„ç†]
    
    subgraph AudioPrep["éŸ³é¢‘å‡†å¤‡"]
        E --> E1[numpy â†’ torch.Tensor]
        E1 --> E2[æ·»åŠ  batch ç»´åº¦<br/>unsqueeze 0]
        E2 --> E3[æ„å»ºéŸ³é¢‘å­—å…¸<br/>waveform + sample_rate]
    end
    
    E3 --> F[æ‰§è¡Œè¯´è¯äººåˆ†ç¦»]
    
    subgraph Diarize["pyannote Pipeline å†…éƒ¨æµç¨‹"]
        F --> F1["1ï¸âƒ£ VAD è¯­éŸ³æ´»åŠ¨æ£€æµ‹<br/>pyannote/segmentation-3.0"]
        F1 --> F2["2ï¸âƒ£ è¯´è¯äººåµŒå…¥æå–<br/>speechbrain/spkrec-ecapa-voxceleb"]
        F2 --> F3["3ï¸âƒ£ èšç±»åˆ†æ<br/>AgglomerativeClustering"]
        F3 --> F4["4ï¸âƒ£ é‡å è¯­éŸ³æ£€æµ‹<br/>Overlapped Speech Detection"]
        F4 --> F5["5ï¸âƒ£ è¾¹ç•Œä¼˜åŒ–<br/>Segmentation Refinement"]
    end
    
    F5 --> G[Diarization ç»“æœ<br/>Annotation å¯¹è±¡]
    
    subgraph Convert["ç»“æœè½¬æ¢"]
        G --> G1[itertracks yield_label=True]
        G1 --> G2[è½¬æ¢ä¸º DataFrame]
        G2 --> G3[æå– start/end/speaker]
    end
    
    G3 --> H[whisperx.assign_word_speakers]
    
    subgraph Assign["è¯´è¯äººåˆ†é…åˆ°è¯"]
        H --> H1[éå† WhisperX ç»“æœ]
        H1 --> H2[æ¯ä¸ª segment/word<br/>è®¡ç®—æ—¶é—´èŒƒå›´]
        H2 --> H3[ä¸ Diarization æ—¶é—´æ®µ<br/>è®¡ç®—é‡å ç‡]
        H3 --> H4[åˆ†é…æœ€å¤§é‡å çš„ speaker]
    end
    
    H4 --> I[è¾“å‡º: å¸¦ speaker æ ‡ç­¾çš„ç»“æœ]
    
    subgraph Cleanup["èµ„æºæ¸…ç†"]
        I --> I1[åˆ é™¤ diarize_model]
        I1 --> I2[torch.cuda.empty_cache]
    end
    
    I2 --> J[cleaned_chunks.xlsx<br/>åŒ…å« speaker åˆ—]
    Z1 --> J
    Z2 --> J
```

#### è¯´è¯äººè¯†åˆ«ä¸å£°çº¹åº“ï¼ˆQdrantï¼‰

> **ğŸ“Œ è¯´æ˜**
> - è¯´è¯äººåˆ†ç¦»åªç»™å‡ºåŒ¿åæ ‡ç­¾ï¼ˆå¦‚ `SPEAKER_00`ï¼‰ã€‚
> - è¯´è¯äººè¯†åˆ«ä¼šå°†åŒ¿åæ ‡ç­¾æ˜ å°„åˆ°è§’è‰²åã€‚
> - è‹¥ `speaker_samples/` ä¸ºç©ºï¼Œå¯è‡ªåŠ¨ä»åˆ†ç¦»ç»“æœä¸­æå–æœ€é•¿ç‰‡æ®µç”Ÿæˆæ ·æœ¬ã€‚

**æµç¨‹è¦ç‚¹ï¼š**
1. ä½¿ç”¨ `pyannote/wespeaker-voxceleb-resnet34-LM` æå–å£°çº¹ embeddingã€‚
2. å‚è€ƒæ ·æœ¬å†™å…¥ Qdrantï¼ˆå¦‚æœå¯ç”¨ `speaker_vector_db`ï¼‰ã€‚
3. è¯†åˆ«æ—¶ä¼˜å…ˆä» Qdrant æ£€ç´¢æœ€ç›¸ä¼¼å£°çº¹ï¼Œå†å›å†™åˆ° `segment/word.speaker`ã€‚

**Qdrant å­˜å‚¨ç»“æ„ï¼š**
- **Collection**: `speaker_embeddings`ï¼ˆå¯é…ç½®ï¼‰
- **Point ID**: UUIDï¼ˆç”±è§’è‰²åæ´¾ç”Ÿï¼‰
- **Vector**: å£°çº¹ embeddingï¼ˆflatten åçš„æµ®ç‚¹æ•°ç»„ï¼‰
- **Payload**: `{ "speaker": "è§’è‰²å" }`

#### pyannote-audio 4.0 Pipeline è¯¦è§£

```mermaid
flowchart LR
    subgraph Input["è¾“å…¥"]
        A["éŸ³é¢‘æ–‡ä»¶/Tensor<br/>{'waveform': tensor, 'sample_rate': 16000}"]
    end
    
    subgraph Stage1["é˜¶æ®µ 1: è¯­éŸ³æ£€æµ‹"]
        B1["Segmentation Model<br/>pyannote/segmentation-3.0"]
        B2["è¾“å‡º: è¯­éŸ³/éè¯­éŸ³æ—¶é—´æ®µ<br/>+ é‡å æ£€æµ‹"]
    end
    
    subgraph Stage2["é˜¶æ®µ 2: åµŒå…¥æå–"]
        C1["Embedding Model<br/>speechbrain/spkrec-ecapa-voxceleb"]
        C2["æ¯ä¸ªè¯­éŸ³æ®µ â†’ 512ç»´å‘é‡"]
    end
    
    subgraph Stage3["é˜¶æ®µ 3: èšç±»"]
        D1["å±‚æ¬¡èšç±»<br/>AgglomerativeClustering"]
        D2["ç›¸ä¼¼åº¦é˜ˆå€¼åˆ¤æ–­<br/>åˆå¹¶åŒä¸€è¯´è¯äºº"]
    end
    
    subgraph Stage4["é˜¶æ®µ 4: åå¤„ç†"]
        E1["è¾¹ç•Œä¼˜åŒ–"]
        E2["é‡å åŒºåŸŸå¤„ç†"]
        E3["æœ€å°æ—¶é•¿è¿‡æ»¤"]
    end
    
    subgraph Output["è¾“å‡º"]
        F["Annotation å¯¹è±¡<br/>[(start, end, speaker_id), ...]"]
    end
    
    A --> B1 --> B2 --> C1 --> C2 --> D1 --> D2 --> E1 --> E2 --> E3 --> F
```

#### è¯´è¯äººåˆ†ç¦»ä¾èµ–çš„æ¨¡å‹

| æ¨¡å‹åç§° | HuggingFace åœ°å€ | ç”¨é€” | æ˜¯å¦ Gated | æ¨¡å‹å¤§å° |
|---------|-----------------|------|-----------|---------|
| speaker-diarization-3.1 | `pyannote/speaker-diarization-3.1` | ä¸» Pipeline é…ç½® | âœ… éœ€åŒæ„æ¡æ¬¾ | ~1KB (é…ç½®æ–‡ä»¶) |
| segmentation-3.0 | `pyannote/segmentation-3.0` | VAD + é‡å æ£€æµ‹ | âœ… éœ€åŒæ„æ¡æ¬¾ | ~5MB |
| spkrec-ecapa-voxceleb | `speechbrain/spkrec-ecapa-voxceleb` | è¯´è¯äººåµŒå…¥ (ECAPA-TDNN) | âŒ | ~80MB |

> **ğŸ“Œ æ³¨æ„**: pyannote-audio 4.0 é»˜è®¤ä½¿ç”¨ `speechbrain/spkrec-ecapa-voxceleb` ä½œä¸ºåµŒå…¥æ¨¡å‹ï¼Œ
> æ›¿ä»£äº†ä¹‹å‰ç‰ˆæœ¬çš„ `wespeaker-voxceleb-resnet34-LM`ã€‚

#### ä»£ç å®ç°ç»†èŠ‚

```python
# 1. åŠ è½½ Pipeline (whisperX_local.py)
from pyannote.audio import Pipeline
diarize_model = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    token=hf_token  # HuggingFace Token
)
diarize_model = diarize_model.to(torch.device(device))  # GPU åŠ é€Ÿ

# 2. å‡†å¤‡éŸ³é¢‘è¾“å…¥
waveform = torch.from_numpy(raw_audio_segment).unsqueeze(0)
audio_dict = {"waveform": waveform, "sample_rate": 16000}

# 3. æ‰§è¡Œè¯´è¯äººåˆ†ç¦»
diarize_result = diarize_model(audio_dict)

# 4. è½¬æ¢ç»“æœä¸º DataFrame
diarize_df = pd.DataFrame(
    diarization.itertracks(yield_label=True), 
    columns=['segment', 'label', 'speaker']
)
diarize_df['start'] = diarize_df['segment'].apply(lambda x: x.start)
diarize_df['end'] = diarize_df['segment'].apply(lambda x: x.end)

# 5. åˆ†é…è¯´è¯äººåˆ°æ¯ä¸ªè¯
result = whisperx.assign_word_speakers(diarize_df, result)
```

#### é…ç½®å‚æ•°

| å‚æ•° | é…ç½®é”® | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|-------|------|
| å¯ç”¨è¯´è¯äººåˆ†ç¦» | `speaker_diarization` | `false` | æ˜¯å¦å¯ç”¨ pyannote è¯´è¯äººåˆ†ç¦» |
| HuggingFace Token | `hf_token` | ç©º | è®¿é—® gated æ¨¡å‹éœ€è¦çš„ token |
| HuggingFace é•œåƒ | `hf_mirror` | ç©º | å›½å†…ç”¨æˆ·å¯è®¾ç½®ä¸º `https://hf-mirror.com` |

#### é¦–æ¬¡ä½¿ç”¨é…ç½®æ­¥éª¤

1. è®¿é—® https://huggingface.co/settings/tokens åˆ›å»º Tokenï¼ˆé€‰æ‹© "Read" æƒé™ï¼‰
2. è®¿é—®ä»¥ä¸‹é¡µé¢å¹¶ç‚¹å‡» "Agree" åŒæ„æ¡æ¬¾ï¼š
   - https://huggingface.co/pyannote/speaker-diarization-3.1
   - https://huggingface.co/pyannote/segmentation-3.0
3. åœ¨ `config.yaml` ä¸­é…ç½®ï¼š
   ```yaml
   hf_token: 'hf_your_token_here'
   speaker_diarization: true
   hf_mirror: 'https://hf-mirror.com'  # å›½å†…ç”¨æˆ·å¯é€‰
   ```

#### æ¨¡å‹ç¼“å­˜ä½ç½®

- Windows: `C:\Users\<ç”¨æˆ·å>\.cache\huggingface\hub\`
- Linux/Mac: `~/.cache/huggingface/hub/`

#### ä¾èµ–ç‰ˆæœ¬

| åŒ…å | ç‰ˆæœ¬ | è¯´æ˜ |
|-----|------|------|
| pyannote-audio | 4.0.3 | ä¸»åº“ |
| pyannote-core | 6.0.1 | æ ¸å¿ƒæ•°æ®ç»“æ„ |
| pyannote-pipeline | 4.0.0 | Pipeline æ¡†æ¶ |
| speechbrain | - | è¯´è¯äººåµŒå…¥æ¨¡å‹ |
| whisperx | - | æ—¶é—´æˆ³åˆ†é… |

> **æ³¨æ„**ï¼šé¦–æ¬¡è¿è¡Œéœ€è¦è”ç½‘ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 85MBï¼‰ï¼Œåç»­è¿è¡Œç›´æ¥ä»æœ¬åœ°ç¼“å­˜åŠ è½½ã€‚
> GPU åŠ é€Ÿæ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦ï¼Œå»ºè®®ä½¿ç”¨ CUDA è®¾å¤‡ã€‚

### Step 3.1: æ–‡æœ¬ç²—åˆ‡åˆ†ï¼ˆNLP é¢„å¤„ç†ï¼‰

> **ğŸ“Œ æ³¨æ„ï¼šè¿™ä¸€æ­¥ä¸æ˜¯çœŸæ­£çš„"åˆ†å¥"ï¼Œè€Œæ˜¯æ–‡æœ¬ç²—åˆ‡åˆ†**
> 
> Step 3.1 çš„ç›®çš„æ˜¯å°† ASR è¾“å‡ºçš„é•¿æ–‡æœ¬æŒ‰**æ ‡ç‚¹ã€æ—¶é—´é—´éš”ã€è¿æ¥è¯**ç­‰è§„åˆ™è¿›è¡Œ**ç²—åˆ‡åˆ†**ï¼Œ
> ä¸ºåç»­çš„è¯­ä¹‰åˆ†å‰²æä¾›è¾ƒçŸ­çš„æ–‡æœ¬ç‰‡æ®µã€‚spaCy åœ¨è¿™é‡Œä¸»è¦ç”¨äºï¼š
> - **åˆ†è¯ï¼ˆtokenizeï¼‰**ï¼šè®¡ç®—æ–‡æœ¬é•¿åº¦
> - **ä¾å­˜åˆ†æ**ï¼šè¯†åˆ«è¿æ¥è¯ã€è¯æ ¹ç­‰è¯­æ³•ç»“æ„
> 
> **çœŸæ­£çš„æ™ºèƒ½åˆ†å¥åœ¨ Step 3.2 ç”± GPT å®Œæˆã€‚**

```mermaid
flowchart TD
    A[è¾“å…¥: cleaned_chunks.xlsx<br/>å­—ç¬¦çº§æ—¶é—´æˆ³æ•°æ®] --> B[æ‹¼æ¥å…¨éƒ¨æ–‡æœ¬]
    
    B --> C[spaCy æ ‡ç‚¹åˆ†å¥<br/>split_by_mark]
    
    C --> D{å¯ç”¨æ—¶é—´é—´éš”åˆ‡åˆ†?<br/>time_gap_threshold > 0}
    
    D -->|æ˜¯| E[å¯¹æ¯ä¸ª spaCy å¥å­<br/>æ£€æŸ¥å†…éƒ¨æ—¶é—´é—´éš”]
    E --> F[å•è¯æŒç»­æ—¶é—´ > é˜ˆå€¼<br/>æˆ– å•è¯é—´éš” > é˜ˆå€¼]
    F --> G[åœ¨è¶…æ—¶ç‚¹åˆ‡åˆ†]
    
    D -->|å¦| H[ä¿æŒ spaCy åˆ†å¥ç»“æœ]
    
    G --> I[split_by_mark.txt]
    H --> I
    
    I --> J[split_by_comma<br/>æŒ‰é€—å·åˆ‡åˆ†]
    J --> K[split_by_connector<br/>æŒ‰è¿æ¥è¯åˆ‡åˆ†<br/>spaCy ä¾å­˜åˆ†æ]
    K --> L{æ–‡æœ¬é•¿åº¦ >60 tokens?}
    L -->|æ˜¯| M[split_long_by_root<br/>åŠ¨æ€è§„åˆ’æŒ‰è¯æ ¹åˆ‡åˆ†]
    L -->|å¦| N[ä¿æŒåŸæ–‡æœ¬]
    M --> O[split_by_nlp.txt]
    N --> O
```

**æ—¶é—´é—´éš”åˆ‡åˆ†å¤„ç†é¡ºåº**ï¼š

1. **å…ˆ spaCy æ ‡ç‚¹åˆ†å¥**ï¼šä½¿ç”¨ spaCy çš„ `doc.sents` å¯¹å…¨æ–‡è¿›è¡Œæ ‡ç‚¹åˆ†å¥
2. **åæ—¶é—´äºŒæ¬¡åˆ‡åˆ†**ï¼šå¯¹æ¯ä¸ª spaCy åˆ†å‡ºçš„å¥å­ï¼Œæ£€æŸ¥å†…éƒ¨æ˜¯å¦æœ‰è¶…é˜ˆå€¼çš„æ—¶é—´é—´éš”
   - æ£€æŸ¥å•è¯çš„ `duration`ï¼ˆæŒç»­æ—¶é—´ï¼‰ï¼šWhisper ä¼šæŠŠåœé¡¿æ—¶é—´ç®—å…¥å•è¯æŒç»­æ—¶é—´
   - æ£€æŸ¥å•è¯é—´çš„ `gap_to_next`ï¼ˆé—´éš”ï¼‰ï¼šçœŸæ­£çš„å•è¯é—´åœé¡¿
   - å¦‚æœä»»ä¸€å€¼è¶…è¿‡é˜ˆå€¼ï¼Œåœ¨è¯¥ä½ç½®åˆ‡åˆ†

> **ä¸ºä»€ä¹ˆæ˜¯è¿™ä¸ªé¡ºåº**ï¼š
> - å¦‚æœå…ˆæ—¶é—´åˆ‡åˆ†å† spaCy åˆ†å¥ï¼ŒspaCy å¯èƒ½ä¼šå¯¹æ—¶é—´åˆ‡åˆ†äº§ç”Ÿçš„ç‰‡æ®µåšé”™è¯¯çš„äºŒæ¬¡åˆ†å¥
> - ä¾‹å¦‚æ—¥è¯­ spaCy å¯èƒ½æŠŠ"ã‹ã­ä½•..."é”™è¯¯åœ°åˆ‡æˆ"ã‹"å’Œ"ã­ä½•..."
> - å…ˆ spaCy å†æ—¶é—´åˆ‡åˆ†ï¼Œå¯ä»¥ä¿ç•™ spaCy çš„æ ‡ç‚¹è¯†åˆ«èƒ½åŠ›ï¼ŒåŒæ—¶åˆ©ç”¨æ—¶é—´ä¿¡æ¯åšç²¾ç¡®åˆ‡åˆ†

**spaCy åœ¨ Step 3.1 çš„ä½œç”¨**ï¼š

| åŠŸèƒ½ | ç”¨é€” | è¯´æ˜ |
|-----|------|------|
| åˆ†è¯ (tokenize) | è®¡ç®—æ–‡æœ¬é•¿åº¦ | åˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ† |
| ä¾å­˜åˆ†æ (dep) | è¯†åˆ«è¿æ¥è¯ | `that`, `which`, `ã®ã§`, `ãŸã‚` ç­‰ |
| è¯æ€§æ ‡æ³¨ (pos) | è¯†åˆ«è¯æ ¹ | åŠ¨è¯ã€åè¯ç­‰ä½œä¸ºåˆ‡åˆ†ç‚¹ |
| å¥å­è¾¹ç•Œ (sents) | æ ‡ç‚¹åˆ†å¥ | å¯¹å…¨æ–‡åšåˆæ­¥æ ‡ç‚¹åˆ†å¥ |

**æ—¶é—´é—´éš”åˆ‡åˆ†å‚æ•°**ï¼š

| å‚æ•° | é…ç½®é”® | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|-------|------|
| æ—¶é—´é—´éš”é˜ˆå€¼ | `time_gap_threshold` | ç©º (ä¸å¯ç”¨) | å•è¯æŒç»­æ—¶é—´æˆ–é—´éš”è¶…è¿‡æ­¤å€¼(ç§’)æ—¶åœ¨ spaCy å¥å­å†…éƒ¨å†åˆ‡åˆ† |

> **æ—¥è¯­å¤„ç†ä¼˜åŒ–**ï¼šæ—¥è¯­å£è¯­é€šå¸¸æ²¡æœ‰æ˜æ˜¾æ ‡ç‚¹ï¼Œä½† Whisper ASR ä¼šåœ¨è‡ªç„¶åœé¡¿å¤„äº§ç”Ÿè¾ƒé•¿çš„å•è¯æŒç»­æ—¶é—´ã€‚
> è®¾ç½® `time_gap_threshold: 3` å¯ä»¥åˆ©ç”¨è¿™äº›åœé¡¿ç‚¹è¿›è¡Œåˆ‡åˆ†ã€‚
> 
> **æ³¨æ„**ï¼šWhisper é€šå¸¸æŠŠåœé¡¿æ—¶é—´ç®—å…¥å‰ä¸€ä¸ªå•è¯çš„ `duration`ï¼Œè€Œä¸æ˜¯ `gap_to_next`ã€‚
> å› æ­¤ä»£ç åŒæ—¶æ£€æŸ¥è¿™ä¸¤ä¸ªå€¼ï¼Œç¡®ä¿ä¸é—æ¼ä»»ä½•åœé¡¿ç‚¹ã€‚

**æ–‡ä»¶æµ**ï¼š

```
cleaned_chunks.xlsx     â† ASR è¾“å‡ºï¼ˆå­—ç¬¦çº§æ—¶é—´æˆ³ï¼‰
    â†“ split_by_mark()       
    â”‚   1. æ‹¼æ¥å…¨æ–‡
    â”‚   2. spaCy æ ‡ç‚¹åˆ†å¥
    â”‚   3. (å¯é€‰) æŒ‰æ—¶é—´é—´éš”äºŒæ¬¡åˆ‡åˆ†
split_by_mark.txt (ä¸´æ—¶)
    â†“ split_by_comma_main() æŒ‰é€—å·åˆ‡åˆ†
split_by_comma.txt (ä¸´æ—¶)
    â†“ split_sentences_main() æŒ‰è¿æ¥è¯åˆ‡åˆ†
split_by_connector.txt (ä¸´æ—¶)
    â†“ split_long_by_root_main() æŒ‰è¯æ ¹åˆ‡åˆ†è¶…é•¿æ–‡æœ¬
split_by_nlp.txt        â† Step 3.1 æœ€ç»ˆè¾“å‡ºï¼ˆç²—åˆ‡åˆ†ç»“æœï¼‰
```

### Step 3.2: è¯­ä¹‰åˆ†å¥ï¼ˆGPT æ™ºèƒ½åˆ†å‰²ï¼‰

> **ğŸ“Œ è¿™ä¸€æ­¥æ‰æ˜¯çœŸæ­£çš„"åˆ†å¥"**
> 
> Step 3.2 ä½¿ç”¨ **GPT è¿›è¡Œè¯­ä¹‰ç†è§£**ï¼Œå°†ç²—åˆ‡åˆ†çš„æ–‡æœ¬ç‰‡æ®µè¿›ä¸€æ­¥åˆ†å‰²æˆ**è¯­ä¹‰å®Œæ•´çš„å¥å­**ã€‚
> spaCy åœ¨è¿™é‡Œåªç”¨äº **åˆ†è¯ï¼ˆtokenizeï¼‰** æ¥è®¡ç®—æ–‡æœ¬é•¿åº¦ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨ GPTã€‚

```mermaid
flowchart TD
    A[è¾“å…¥: split_by_nlp.txt<br/>ç²—åˆ‡åˆ†æ–‡æœ¬] --> B[åŠ è½½ spaCy æ¨¡å‹]
    B --> C[éå†æ¯ä¸ªæ–‡æœ¬ç‰‡æ®µ]
    C --> D[spaCy tokenize<br/>è®¡ç®— token æ•°é‡]
    
    D --> E{token æ•° > max_split_length?}
    E -->|â‰¤ 20 tokens| F[ä¿æŒåŸæ–‡æœ¬<br/>é•¿åº¦åˆé€‚æ— éœ€åˆ†å‰²]
    E -->|> 20 tokens| G[éœ€è¦ GPT åˆ†å¥]
    
    G --> H[è®¡ç®—åˆ†å‰²æ•°<br/>num_parts = ceil tokens/20]
    H --> I[è°ƒç”¨ GPT API]
    
    subgraph GPT["GPT è¯­ä¹‰åˆ†å¥"]
        I --> I1[Prompt: å°†å¥å­åˆ†æˆ N éƒ¨åˆ†]
        I1 --> I2[GPT ç†è§£è¯­ä¹‰]
        I2 --> I3[è¿”å›: å¥å­1 âˆ¥ å¥å­2 âˆ¥ å¥å­3]
    end
    
    I3 --> J[find_split_positions<br/>åœ¨åŸæ–‡ä¸­å®šä½åˆ†å‰²ç‚¹]
    
    subgraph Align["åˆ†å‰²ç‚¹å¯¹é½"]
        J --> J1[è®¡ç®— GPT ç»“æœä¸åŸæ–‡çš„ç›¸ä¼¼åº¦]
        J1 --> J2{ç›¸ä¼¼åº¦ > 0.9?}
        J2 -->|æ˜¯| J3[è®°å½•åˆ†å‰²ä½ç½®]
        J2 -->|å¦| J4[Warning + å°½åŠ›åŒ¹é…]
    end
    
    J3 --> K[åœ¨åˆ†å‰²ç‚¹æ’å…¥æ¢è¡Œ]
    J4 --> K
    F --> L[split_by_meaning.txt]
    K --> L
    
    L --> M{è¿˜æœ‰è¶…é•¿å¥å­?}
    M -->|æ˜¯| N[é€’å½’å¤„ç†<br/>æœ€å¤š 3 æ¬¡]
    M -->|å¦| O[å®Œæˆ]
    N --> C
```

**spaCy åœ¨ Step 3.2 çš„ä½œç”¨**ï¼š

| åŠŸèƒ½ | ç”¨é€” |
|-----|------|
| **åˆ†è¯ (tokenize)** | è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡ï¼Œåˆ¤æ–­æ˜¯å¦è¶…è¿‡é˜ˆå€¼éœ€è¦ GPT åˆ†å¥ |

> **æ³¨æ„**ï¼šStep 3.2 ä¸­ spaCy **ä¸åšåˆ†å¥**ï¼Œåˆ†å¥å®Œå…¨ç”± GPT å®Œæˆã€‚

**è¯­ä¹‰åˆ†å¥å…³é”®å‚æ•°**:

| å‚æ•° | é…ç½®é”® | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|-------|------|
| æœ€å¤§åˆ†å‰²é•¿åº¦ | `max_split_length` | 20 | è¶…è¿‡æ­¤ token æ•°è§¦å‘ GPT åˆ†å¥ |
| æ—¶é—´é—´éš”é˜ˆå€¼ | `time_gap_threshold` | ç©º (ä¸å¯ç”¨) | Step 3.1 ä¸­æŒ‰æ—¶é—´åˆ‡åˆ†çš„é˜ˆå€¼(ç§’) |
| å¹¶å‘æ•° | `max_workers` | 4 | GPT è¯·æ±‚å¹¶å‘æ•° |
| ç›¸ä¼¼åº¦é˜ˆå€¼ | - | 0.9 | åˆ†å‰²ç‚¹å®šä½çš„æœ€å°ç›¸ä¼¼åº¦ |
| æœ€å¤§é‡è¯•æ¬¡æ•° | - | 3 | é€’å½’å¤„ç†è¶…é•¿å¥å­çš„æ¬¡æ•° |

**è¯­è¨€æ¨¡å‹é€‰æ‹©é€»è¾‘**:

```python
# init_nlp() è¯­è¨€é€‰æ‹© - ç”¨äºåˆ†è¯
user_language = load_key("whisper.language")      # ç”¨æˆ·è®¾ç½®çš„è¯­è¨€
detected_language = load_key("whisper.detected_language")  # è‡ªåŠ¨æ£€æµ‹çš„è¯­è¨€
language = user_language if user_language else detected_language

# æ˜ å°„åˆ° spaCy æ¨¡å‹ï¼ˆç”¨äºåˆ†è¯ï¼Œä¸æ˜¯åˆ†å¥ï¼‰
SPACY_MODEL_MAP = {
    "ja": "ja_core_news_md",
    "en": "en_core_web_md", 
    "zh": "zh_core_web_md",
    ...
}
```

**GPT åˆ†å‰² Prompt ç¤ºä¾‹**:

```
è¯·å°†ä»¥ä¸‹å¥å­åˆ†æˆ 3 éƒ¨åˆ†ï¼Œç”¨ || åˆ†éš”:
"é«˜ãƒ¬ãƒ™ãƒ«ã®è­¦æˆ’éš è”½ã‚’ä½¿ã†ã“ã¨ã¯ãƒ¨ã‚¬ãƒ©ã‚¹ã®ã‚«ãƒ¡ãƒ©ã‚’é€šã—ã¦è¦‹ã¦ã„ãŸã®ã§ãªãŠå‰ãŒç‹å¥³ã«ã¤ãã¾ã¨ã£ã¦ã„ã‚‹ã¨çŸ¥ã‚Šã‚µãƒ©ãƒ ã®é­”çœ¼ã«ä¼¼ã›ãŸä»•çµ„ã¿ã‚’ä½œã‚‰ã›ãŸã®ã "

GPT è¿”å›:
"é«˜ãƒ¬ãƒ™ãƒ«ã®è­¦æˆ’éš è”½ã‚’ä½¿ã†ã“ã¨ã¯ãƒ¨ã‚¬ãƒ©ã‚¹ã®ã‚«ãƒ¡ãƒ©ã‚’é€šã—ã¦è¦‹ã¦ã„ãŸã®ã§||ãªãŠå‰ãŒç‹å¥³ã«ã¤ãã¾ã¨ã£ã¦ã„ã‚‹ã¨çŸ¥ã‚Š||ã‚µãƒ©ãƒ ã®é­”çœ¼ã«ä¼¼ã›ãŸä»•çµ„ã¿ã‚’ä½œã‚‰ã›ãŸã®ã "
```

### Step 4.2: ç¿»è¯‘åŒæ­¥éª¤æµç¨‹

> **âš ï¸ CJK æ¨¡å¼çš„åˆ†å¥"ç ´å"ä¸é‡å»º**
>
> å¯¹äº CJK è¯­è¨€ï¼ˆæ—¥è¯­ jaã€ä¸­æ–‡ zhã€éŸ©è¯­ koï¼‰ï¼ŒStep 4.2 ä¼š**æ‰“ç ´ Step 3.2 çš„åˆ†å¥ç»“æ„**ï¼š
> - Step 3.2 ç²¾å¿ƒåˆ†å‰²çš„è¯­ä¹‰å¥å­åœ¨ Step 4.2 è¢«**åˆå¹¶æˆå¤§å—**å‘é€ç»™ LLM ç¿»è¯‘
> - ç¿»è¯‘ç»“æœæŒ‰ LLM è‡ªç„¶æ¢è¡Œåˆ†å‰²ï¼Œ**ä¸å†ä¸åŸæ–‡è¡Œæ•°å¯¹åº”**
> - åŸæ–‡å­—ç¬¦è¢«**æŒ‰å­—ç¬¦æ•°å‡åŒ€åˆ†é…**åˆ°ç¿»è¯‘è¡Œä¸­ï¼ˆç”¨äºæ˜¾ç¤ºå¯¹ç…§ï¼Œéè¯­ä¹‰å¯¹åº”ï¼‰
> - æ—¶é—´æˆ³ä¹Ÿè¢«**å‡åŒ€åˆ†é…**åˆ°æ–°çš„ç¿»è¯‘è¡Œä¸Š
>
> **ç›®çš„**ï¼šCJK è¯­è¨€çš„ç‰¹ç‚¹æ˜¯æ²¡æœ‰ç©ºæ ¼åˆ†è¯ï¼ŒStep 3.2 çš„åˆ†å¥ç»“æœå¯èƒ½åœ¨ç¿»è¯‘åäº§ç”Ÿä¸è‡ªç„¶çš„æ–­å¥ã€‚
> è®© LLM åœ¨ç¿»è¯‘æ—¶è‡ªä¸»å†³å®šå¦‚ä½•æ–­å¥ï¼Œå¯ä»¥è·å¾—æ›´æµç•…çš„ç›®æ ‡è¯­è¨€å­—å¹•ã€‚
>
> **åæœ**ï¼šåŸæ–‡ä¸è¯‘æ–‡çš„**è¡Œçº§å¯¹åº”å…³ç³»è¢«ç ´å**ï¼Œä½† Step 5 ä¼šå†æ¬¡åŸºäºå­—å¹•é•¿åº¦é™åˆ¶è¿›è¡Œåˆ†å‰²å¯¹é½ã€‚

```mermaid
flowchart TD
    A[è¾“å…¥æ–‡æœ¬] --> B["Step 1: å¿ å®ç¿»è¯‘<br/>(ç›´è¯‘åŸæ–‡ã€ä¿æŒåŸæ„ã€å‚è€ƒæœ¯è¯­è¡¨)"]
    B --> C{reflect_translate?}
    C -->|true| D["Step 2: è¾¾æ„ç¿»è¯‘<br/>(æ¶¦è‰²è¡¨è¾¾ã€ä¼˜åŒ–è¯­åºã€é€‚åº”ç›®æ ‡è¯­è¨€)"]
    C -->|false| E[è¾“å‡ºå¿ å®ç¿»è¯‘ç»“æœ]
    D --> F[è¾“å‡ºè¾¾æ„ç¿»è¯‘ç»“æœ]
```

#### CJK æ¨¡å¼æŠ€æœ¯å®ç°

```mermaid
flowchart TD
    A[split_by_meaning.txt<br/>Step 3.2 åˆ†å¥ç»“æœ] --> B{æ£€æµ‹æºè¯­è¨€}
    
    B -->|é CJK| C[ä¿æŒåŸè¡Œç»“æ„ç¿»è¯‘]
    B -->|CJK: ja/zh/ko| D[CJK æ¨¡å¼]
    
    subgraph CJK["CJK æ¨¡å¼å¤„ç†"]
        D --> D1[åˆå¹¶å¤šè¡Œæˆå¤§å—<br/>600å­—ç¬¦/10è¡Œä¸Šé™]
        D1 --> D2[å‘é€ç»™ LLM ç¿»è¯‘]
        D2 --> D3[è·å–ç¿»è¯‘ç»“æœ<br/>LLM è‡ªç„¶æ–­è¡Œ]
        D3 --> D4[åŸæ–‡æŒ‰å­—ç¬¦æ•°<br/>å‡åŒ€åˆ†é…åˆ°è¯‘æ–‡è¡Œ]
        D4 --> D5[æ—¶é—´æˆ³æŒ‰è¯‘æ–‡è¡Œæ•°<br/>å‡åŒ€åˆ†é…]
    end
    
    subgraph NonCJK["é CJK æ¨¡å¼å¤„ç†"]
        C --> C1[é€è¡Œç¿»è¯‘ä¿æŒå¯¹åº”]
        C1 --> C2[ç›¸ä¼¼åº¦åŒ¹é…éªŒè¯]
        C2 --> C3[åŸè¡Œ â†” è¯‘è¡Œ 1:1 å¯¹åº”]
    end
    
    D5 --> E[translation.xlsx]
    C3 --> E
    
    E --> F[Step 5: å­—å¹•åˆ†å‰²<br/>åŸºäº subtitle.max_length å†æ¬¡åˆ‡åˆ†]
```

**CJK æ¨¡å¼å…³é”®ä»£ç é€»è¾‘**:

```python
# æ£€æµ‹æ˜¯å¦ä¸º CJK è¯­è¨€
cjk_languages = ['ja', 'zh', 'ko', 'japanese', 'chinese', 'korean']
is_cjk = detected_language.lower() in cjk_languages

if is_cjk:
    # åŸæ–‡å­—ç¬¦å‡åŒ€åˆ†é…åˆ°è¯‘æ–‡è¡Œ
    chars_per_line = len(src_block) // len(trans_lines)
    src_text.append(src_block[start_idx:end_idx])
    
    # æ—¶é—´æˆ³å‡åŒ€åˆ†é…
    duration_per_line = total_duration / num_lines
```

### Step 5: å­—å¹•åˆ†å‰²å¯¹é½ï¼ˆé‡å»ºåˆ†å¥ç»“æ„ï¼‰

> **ğŸ“Œ Step 5 çš„æ ¸å¿ƒä½œç”¨**
>
> Step 5 åŸºäº**æ˜¾ç¤ºé•¿åº¦é™åˆ¶**é‡æ–°åˆ‡åˆ†å­—å¹•ï¼Œç¡®ä¿æ¯è¡Œå­—å¹•ä¸è¶…è¿‡ `subtitle.max_length` å­—ç¬¦ã€‚
> è¿™ä¸€æ­¥å¯¹äº CJK æ¨¡å¼å°¤å…¶é‡è¦ï¼Œå› ä¸º Step 4.2 å·²ç»ç ´åäº†åŸæœ‰çš„åˆ†å¥ç»“æ„ã€‚

```mermaid
flowchart TD
    A[translation.xlsx<br/>Step 4.2 ç¿»è¯‘ç»“æœ] --> B[éå†æ¯è¡Œå­—å¹•]
    
    B --> C{è®¡ç®—æ˜¾ç¤ºé•¿åº¦<br/>calc_lenè€ƒè™‘CJKæƒé‡}
    
    C -->|åŸæ–‡ > max_length<br/>æˆ– è¯‘æ–‡Ã—1.2 > max_length| D[éœ€è¦åˆ†å‰²]
    C -->|é•¿åº¦åˆé€‚| E[ä¿æŒåŸæ ·]
    
    subgraph Split["GPT åˆ†å‰²å¤„ç†"]
        D --> D1[è°ƒç”¨ split_sentence<br/>ä¸ Step 3.2 ç›¸åŒå‡½æ•°]
        D1 --> D2[GPT åˆ†æˆ 2 éƒ¨åˆ†]
        D2 --> D3[align_subs å¯¹é½<br/>åŸæ–‡ä¸è¯‘æ–‡åŒæ­¥åˆ†å‰²]
    end
    
    D3 --> F{è¿˜æœ‰è¶…é•¿è¡Œ?}
    F -->|æ˜¯| G[é€’å½’å¤„ç†<br/>æœ€å¤š 3 æ¬¡]
    G --> B
    F -->|å¦| H[è¾“å‡º split_sub.xlsx]
    E --> H
```

**å­—å¹•é•¿åº¦è®¡ç®—æƒé‡**ï¼š

```python
def calc_len(text: str) -> float:
    """è®¡ç®—å­—å¹•æ˜¾ç¤ºé•¿åº¦ï¼Œè€ƒè™‘ä¸åŒå­—ç¬¦å®½åº¦"""
    # ä¸­æ—¥æ–‡å­—ç¬¦æƒé‡ 1.75
    # éŸ©æ–‡å­—ç¬¦æƒé‡ 1.5
    # æ³°æ–‡å­—ç¬¦æƒé‡ 1.0
    # å…¨è§’ç¬¦å·æƒé‡ 1.75
    # è‹±æ–‡å’ŒåŠè§’ç¬¦å·æƒé‡ 1.0
```

**Step 5 å…³é”®å‚æ•°**ï¼š

| å‚æ•° | é…ç½®é”® | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|-------|-------|------|
| å­—å¹•æœ€å¤§é•¿åº¦ | `subtitle.max_length` | 75 | æ¯è¡Œå­—å¹•çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆè€ƒè™‘æƒé‡åï¼‰ |
| è¯‘æ–‡é•¿åº¦å€æ•° | `subtitle.target_multiplier` | 1.2 | è¯‘æ–‡é€šå¸¸æ¯”åŸæ–‡é•¿ï¼Œä¹˜ä»¥æ­¤å€æ•°ååˆ¤æ–­æ˜¯å¦è¶…é•¿ |

### Step 3-4-5 åˆ†å¥æµç¨‹æ€»è§ˆ

```mermaid
flowchart LR
    subgraph Step3["Step 3: åˆæ¬¡åˆ†å¥"]
        A1["3.1 NLP ç²—åˆ†"] --> A2["3.2 GPT è¯­ä¹‰åˆ†å¥"]
    end
    
    subgraph Step4["Step 4: ç¿»è¯‘"]
        B1["4.1 æœ¯è¯­æå–"] --> B2["4.2 ç¿»è¯‘"]
        B2 --> B3{CJK?}
        B3 -->|æ˜¯| B4["ç ´ååˆ†å¥ç»“æ„<br/>LLM è‡ªç„¶æ–­è¡Œ"]
        B3 -->|å¦| B5["ä¿æŒ 1:1 å¯¹åº”"]
    end
    
    subgraph Step5["Step 5: é‡å»ºåˆ†å¥"]
        C1["æ£€æŸ¥æ¯è¡Œé•¿åº¦"] --> C2{è¶…é•¿?}
        C2 -->|æ˜¯| C3["GPT åˆ†å‰² + å¯¹é½"]
        C2 -->|å¦| C4["ä¿æŒåŸæ ·"]
    end
    
    A2 --> B1
    B4 --> C1
    B5 --> C1
    C3 --> D["split_sub.xlsx<br/>æœ€ç»ˆå­—å¹•åˆ†å¥"]
    C4 --> D
    
    style B4 fill:#ffcccc,stroke:#cc0000
    style C3 fill:#ccffcc,stroke:#00cc00
```

> **è®¾è®¡æ„å›¾æ€»ç»“**ï¼š
> - **Step 3.2**: åŸºäºè¯­ä¹‰çš„"ç²—åˆ†"ï¼Œä¸ºç¿»è¯‘æä¾›åˆç†çš„ä¸Šä¸‹æ–‡å•å…ƒ
> - **Step 4.2 CJK æ¨¡å¼**: æ‰“ç ´åˆ†å¥ï¼Œè®© LLM ç¿»è¯‘æ—¶è‡ªç„¶æ–­è¡Œï¼Œè·å¾—æµç•…çš„ç›®æ ‡è¯­è¨€
> - **Step 5**: åŸºäºæ˜¾ç¤ºé•¿åº¦çš„"ç²¾åˆ†"ï¼Œç¡®ä¿å­—å¹•å¯è¯»æ€§ï¼Œä½¿ç”¨åŒæ ·çš„ GPT åˆ†å¥å‡½æ•°é‡å»ºç»“æ„

---

## UML å›¾

### å¤„ç†æµç¨‹åºåˆ—å›¾

```mermaid
sequenceDiagram
    participant User as ç”¨æˆ·
    participant YT as yt-dlp
    participant Demucs as Demucs
    participant Whisper as WhisperX
    participant SpaCy as spaCy
    participant LLM as LLM API
    participant TTS as TTS API
    participant FFmpeg as FFmpeg

    User->>YT: 1. ä¸‹è½½è§†é¢‘
    YT-->>User: video.mp4
    
    User->>Demucs: 2. äººå£°åˆ†ç¦»
    Demucs-->>User: vocal.mp3 + background.mp3
    
    User->>Whisper: 3. è¯­éŸ³è¯†åˆ« (raw.mp3)
    Whisper-->>User: è½¬å½•æ–‡æœ¬
    User->>Whisper: æ—¶é—´æˆ³å¯¹é½ (vocal.mp3)
    Whisper-->>User: cleaned_chunks.xlsx
    
    rect rgb(200, 230, 255)
        Note over SpaCy: Step 3.1 NLP åˆ†å¥
        User->>SpaCy: 4a. æ ‡ç‚¹/æ—¶é—´åˆ†å¥
        SpaCy-->>User: split_by_mark.txt
        User->>SpaCy: 4b. é€—å·åˆ†å‰²
        SpaCy-->>User: split_by_comma.txt
        User->>SpaCy: 4c. è¿æ¥è¯åˆ†å‰²
        SpaCy-->>User: split_by_connector.txt
        User->>SpaCy: 4d. é•¿å¥æŒ‰è¯æ ¹åˆ†å‰²
        SpaCy-->>User: split_by_nlp.txt
    end
    
    rect rgb(255, 230, 200)
        Note over LLM: Step 3.2 è¯­ä¹‰åˆ†å‰²
        User->>SpaCy: 5a. åŠ è½½è¯­è¨€æ¨¡å‹
        SpaCy-->>User: nlp (ja/en/zh...)
        User->>SpaCy: 5b. Tokenize è®¡ç®—é•¿åº¦
        SpaCy-->>User: token æ•°é‡
        User->>LLM: 5c. GPT åˆ†å‰²è¶…é•¿å¥
        LLM-->>User: åˆ†å‰²ç‚¹ (||)
        User->>User: 5d. å®šä½å¹¶åº”ç”¨åˆ†å‰²
        User-->>User: split_by_meaning.txt
    end
    
    User->>LLM: 6. æœ¯è¯­æå–
    LLM-->>User: terminology.json
    
    User->>LLM: 7. ç¿»è¯‘
    LLM-->>User: ç¿»è¯‘ç»“æœ
    
    User->>LLM: 8. å­—å¹•å‹ç¼©
    LLM-->>User: é€‚é…é•¿åº¦çš„å­—å¹•
    
    User->>TTS: 9. ç”Ÿæˆé…éŸ³
    TTS-->>User: éŸ³é¢‘ç‰‡æ®µ
    
    User->>FFmpeg: 10. éŸ³è§†é¢‘åˆæˆ
    FFmpeg-->>User: output_dub.mp4
```

### æ¨¡å—ç±»å›¾

```mermaid
classDiagram
    class Core {
        +_1_ytdlp.py
        +_2_asr.py
        +_3_1_split_nlp.py
        +_3_2_split_meaning.py
        +_4_1_summarize.py
        +_4_2_translate.py
        +_5_split_sub.py
        +_6_gen_sub.py
        +_7_sub_into_vid.py
        +_8_1_audio_task.py
        +_8_2_dub_chunks.py
        +_9_refer_audio.py
        +_10_gen_audio.py
        +_11_merge_audio.py
        +_12_dub_to_vid.py
    }
    
    class ASRBackend {
        +whisperX_local.py
        +whisperX_302.py
        +elevenlabs_asr.py
        +demucs_vl.py
        +audio_preprocess.py
        +transcribe_audio()
        +speaker_diarization()
    }
    
    class SpeakerDiarization {
        +pyannote.audio 4.0.3 Pipeline
        +speaker-diarization-3.1
        +segmentation-3.0
        +speechbrain/spkrec-ecapa-voxceleb
        +AgglomerativeClustering
        +whisperx.assign_word_speakers()
    }
    
    class SpacyUtils {
        +load_nlp_model()
        +split_by_mark()
        +split_by_comma()
        +split_by_connector()
        +split_long_by_root()
    }
    
    class TTSBackend {
        +tts_main.py
        +azure_tts.py
        +openai_tts.py
        +edge_tts.py
        +gpt_sovits_tts.py
        +fish_tts.py
        +sf_cosyvoice2.py
        +generate_audio()
    }
    
    class Utils {
        +ask_gpt()
        +load_key()
        +check_file_exists()
        +except_handler()
        +rprint()
    }
    
    Core --> ASRBackend : uses
    ASRBackend --> SpeakerDiarization : optional
    Core --> SpacyUtils : uses
    Core --> TTSBackend : uses
    Core --> Utils : uses
```

### çŠ¶æ€æœºå›¾

```mermaid
stateDiagram-v2
    [*] --> Download: å¼€å§‹å¤„ç†
    
    Download --> ASR: ä¸‹è½½å®Œæˆ
    Download --> Error: ä¸‹è½½å¤±è´¥
    
    ASR --> NLPSplit: ASRå®Œæˆ
    ASR --> Error: ASRå¤±è´¥
    
    NLPSplit --> LLMSplit: NLPåˆ†å‰²å®Œæˆ
    NLPSplit --> Error: åˆ†å‰²å¤±è´¥
    
    LLMSplit --> Summarize: è¯­ä¹‰åˆ†å‰²å®Œæˆ
    
    Summarize --> Translate: æœ¯è¯­æå–å®Œæˆ
    
    Translate --> Pause: pause_before_translate=true
    Translate --> SubSplit: ç¿»è¯‘å®Œæˆ
    
    Pause --> SubSplit: ç”¨æˆ·ç¡®è®¤ç»§ç»­
    
    SubSplit --> GenSub: å­—å¹•åˆ†å‰²å®Œæˆ
    
    GenSub --> BurnSub: å­—å¹•ç”Ÿæˆå®Œæˆ
    
    BurnSub --> AudioTask: å­—å¹•çƒ§å½•å®Œæˆ
    
    AudioTask --> DubChunks: ä»»åŠ¡ç”Ÿæˆå®Œæˆ
    
    DubChunks --> ReferAudio: åˆ†å—å®Œæˆ
    
    ReferAudio --> TTSGen: å‚è€ƒéŸ³é¢‘æå–å®Œæˆ
    
    TTSGen --> MergeAudio: TTSç”Ÿæˆå®Œæˆ
    TTSGen --> Error: TTSå¤±è´¥
    
    MergeAudio --> DubVid: éŸ³é¢‘åˆå¹¶å®Œæˆ
    
    DubVid --> [*]: å¤„ç†å®Œæˆ
    
    Error --> [*]: å¤„ç†ç»ˆæ­¢
```

---

## æ¨¡å‹ä¸æŠ€æœ¯é€‰å‹

### ASR æ¨¡å‹å¯¹æ¯”

```mermaid
graph LR
    subgraph Local["æœ¬åœ°æ¨¡å‹"]
        A["faster-whisper-large-v3<br/>é«˜ç²¾åº¦"]
        B["faster-whisper-large-v3-turbo<br/>é€Ÿåº¦å¿«"]
        C["Belle-whisper-large-v3-zh<br/>ä¸­æ–‡ä¼˜åŒ–"]
    end
    
    subgraph Cloud["äº‘ç«¯ API"]
        D["302.ai Whisper<br/>æ— éœ€ GPU"]
        E["ElevenLabs ASR<br/>é«˜è´¨é‡"]
    end
    
    subgraph Alignment["æ—¶é—´æˆ³å¯¹é½"]
        F["WhisperX<br/>å•è¯çº§å¯¹é½"]
    end
    
    subgraph Diarization["è¯´è¯äººåˆ†ç¦» (å¯é€‰)"]
        G["pyannote.audio<br/>speaker-diarization-3.1"]
    end
    
    A --> F
    B --> F
    C --> F
    D --> F
    E --> F
    F --> G
```

### è¯´è¯äººåˆ†ç¦»æŠ€æœ¯æ ˆ (pyannote-audio 4.0.3)

| ç»„ä»¶ | æ¨¡å‹/åº“ | HuggingFace åœ°å€ | è¯´æ˜ |
|-----|--------|------------------|------|
| Pipeline | pyannote-audio 4.0.3 | `pyannote/speaker-diarization-3.1` | ä¸» Pipeline é…ç½®æ–‡ä»¶ |
| VAD + OSD | segmentation-3.0 | `pyannote/segmentation-3.0` | è¯­éŸ³æ´»åŠ¨æ£€æµ‹ + é‡å è¯­éŸ³æ£€æµ‹ |
| Speaker Embedding | ECAPA-TDNN | `speechbrain/spkrec-ecapa-voxceleb` | æå–è¯´è¯äºº 512 ç»´ç‰¹å¾å‘é‡ |
| Clustering | AgglomerativeClustering | - | å±‚æ¬¡èšç±»ï¼Œåˆå¹¶åŒä¸€è¯´è¯äºº |
| Speaker Assignment | whisperx | `whisperx.assign_word_speakers` | å°†è¯´è¯äººæ ‡ç­¾åˆ†é…åˆ°æ¯ä¸ªè¯ |

> **ç‰ˆæœ¬å˜æ›´è¯´æ˜**: pyannote-audio 4.0 ä½¿ç”¨ `speechbrain/spkrec-ecapa-voxceleb` (ECAPA-TDNN) æ›¿ä»£äº†
> ä¹‹å‰ç‰ˆæœ¬çš„ `wespeaker-voxceleb-resnet34-LM`ï¼Œæä¾›æ›´å¥½çš„è¯´è¯äººåµŒå…¥è´¨é‡ã€‚

### TTS åŠŸèƒ½æ”¯æŒè¡¨

| å¼•æ“ | è¯­è¨€æ”¯æŒ | å£°éŸ³å…‹éš† | æˆæœ¬ | API æ¥æº |
|-----|---------|---------|------|---------|
| **Azure TTS** | 100+ | âŒ | ä»˜è´¹ | 302.ai |
| **OpenAI TTS** | å¤šè¯­è¨€ | âŒ | ä»˜è´¹ | 302.ai |
| **Edge TTS** | å¤šè¯­è¨€ | âŒ | å…è´¹ | å¾®è½¯ |
| **GPT-SoVITS** | å¤šè¯­è¨€ | âœ… | æœ¬åœ°éƒ¨ç½² | æœ¬åœ° |
| **Fish TTS** | ä¸­/è‹± | âœ… | ä»˜è´¹ | 302.ai / SiliconFlow |
| **CosyVoice2** | ä¸­/è‹± | âœ… | ä»˜è´¹ | SiliconFlow |
| **F5-TTS** | å¤šè¯­è¨€ | âœ… | ä»˜è´¹ | 302.ai |

### NLP æ¨¡å‹æ”¯æŒ

| è¯­è¨€ | spaCy æ¨¡å‹ | ç”¨é€” |
|-----|-----------|------|
| English | `en_core_web_md` | åˆ†è¯ã€å¥æ³•åˆ†æ |
| Chinese | `zh_core_web_md` | ä¸­æ–‡åˆ†è¯ |
| Japanese | `ja_core_news_md` | æ—¥æ–‡åˆ†è¯ |
| German | `de_core_news_md` | å¾·æ–‡åˆ†è¯ |
| French | `fr_core_news_md` | æ³•æ–‡åˆ†è¯ |
| Spanish | `es_core_news_md` | è¥¿ç­ç‰™æ–‡åˆ†è¯ |

---

## æ•°æ®æµå›¾

### æ–‡ä»¶æ•°æ®æµ

```mermaid
flowchart TD
    subgraph Input["è¾“å…¥"]
        V["video.mp4<br/>æˆ– YouTube URL"]
    end
    
    subgraph Audio["output/audio/"]
        A1["raw.mp3"]
        A2["vocal.mp3"]
        A3["background.mp3"]
        A4["refers/*.wav"]
        A5["segs/*.wav"]
        A6["audio_task.xlsx"]
    end
    
    subgraph Log["output/log/"]
        L1["cleaned_chunks.xlsx<br/>(å« speaker åˆ—)"]
        L2["split_by_nlp.txt"]
        L3["split_by_meaning.txt"]
        L4["translation.xlsx"]
        L5["split_sub.xlsx"]
    end
    
    subgraph GPTLog["output/gpt_log/"]
        G1["terminology.json"]
        G2["summary.json"]
    end
    
    subgraph Final["output/"]
        F1["src.srt"]
        F2["trans.srt"]
        F3["src_trans.srt"]
        F4["dub.mp3"]
        F5["output_sub.mp4"]
        F6["output_dub.mp4"]
    end
    
    V -->|Step 2| A1
    A1 -->|Demucs| A2
    A1 -->|Demucs| A3
    A2 -->|WhisperX| L1
    L1 -->|Step 3.1| L2
    L2 -->|Step 3.2| L3
    L3 -->|Step 4.1| G1
    L3 -->|Step 4.1| G2
    L3 -->|Step 4.2| L4
    L4 -->|Step 5| L5
    L5 -->|Step 6| F1
    L5 -->|Step 6| F2
    L5 -->|Step 6| F3
    F2 -->|Step 7| F5
    L5 -->|Step 8| A6
    A2 -->|Step 9| A4
    A6 -->|Step 10| A5
    A5 -->|Step 11| F4
    F4 -->|Step 12| F6
    A3 -->|Step 12| F6
```

### é…ç½®å‚æ•°å…³ç³»å›¾

```mermaid
mindmap
  root((config.yaml))
    APIé…ç½®
      api.key
      api.base_url
      api.model
    è¯­è¨€é…ç½®
      source_language
      target_language
      whisper.method
      whisper.language
    å¤„ç†å‚æ•°
      max_workers
      max_split_length
      time_gap_threshold
      summary_length
      reflect_translate
    TTSé…ç½®
      tts_method
      speed_factor.accept
      speed_factor.min
      voice_character
    å­—å¹•é…ç½®
      subtitle.max_length
      subtitle.target_multiplier
      burn_subtitles
    ç½‘ç»œé…ç½®
      hf_mirror
      http_proxy
    è¯´è¯äººåˆ†ç¦»
      speaker_diarization
      hf_token
```

---

## æ€»ç»“

VideoLingo æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–è®¾è®¡çš„è§†é¢‘æœ¬åœ°åŒ–ç³»ç»Ÿï¼Œå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

```mermaid
mindmap
  root((VideoLingo))
    æ¶æ„ç‰¹ç‚¹
      æµæ°´çº¿æ¶æ„
        12ä¸ªç‹¬ç«‹æ­¥éª¤
        ä¸­é—´æ–‡ä»¶äº§å‡º
        æ–­ç‚¹ç»­ä¼ æ”¯æŒ
      å¤šæ¨¡å‹æ”¯æŒ
        ASRå¼•æ“åˆ‡æ¢
        TTSå¼•æ“åˆ‡æ¢
        LLMæ¨¡å‹åˆ‡æ¢
    æ ¸å¿ƒèƒ½åŠ›
      æ™ºèƒ½å¤„ç†
        LLMè¯­ä¹‰åˆ†å‰²
        åŒæ­¥éª¤ç¿»è¯‘
        æœ¯è¯­ä¸€è‡´æ€§
      æ€§èƒ½ä¼˜åŒ–
        å¹¶è¡Œå¤„ç†
        GPUåŠ é€Ÿ
        ç¼“å­˜æœºåˆ¶
    é…ç½®çµæ´»
      config.yaml
      APIå¯†é’¥ç®¡ç†
      å¤šè¯­è¨€æ”¯æŒ
```

### æŠ€æœ¯æ ˆæ€»è§ˆ

```mermaid
pie title æŠ€æœ¯æ ˆç»„æˆ
    "Python Core" : 40
    "AI/ML Models" : 25
    "FFmpeg/Audio" : 15
    "FastAPI/React" : 12
    "External APIs" : 8
```
